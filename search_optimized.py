import pandas as pd
from docx import Document
import asyncio
import uuid
import aiohttp
import json
import re
from io import BytesIO
from typing import List, Dict, Tuple, Optional
from difflib import SequenceMatcher
import logging
import os
import string
import time

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —É–º–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä –∏ –ø–æ–¥—Ä–æ–±–Ω—É—é –æ—Ç—á—ë—Ç–Ω–æ—Å—Ç—å
from smart_filter import SmartPhraseFilter
# from detailed_report import format_detailed_report

AUTH_KEY = 'ZGMzMGJmZjEtODQwYS00ZjAwLWI2NjgtNGIyNGNiY2ViNmE1OjYwNjM3NTU0LWQxMDctNDA5ZS1hZWM3LTAwYjQ5MjZkOGU2OA=='
SCOPE = 'GIGACHAT_API_PERS'
API_AUTH_URL = 'https://ngw.devices.sberbank.ru:9443/api/v2/oauth'
API_CHAT_URL = 'https://gigachat.devices.sberbank.ru/api/v1/chat/completions'
MAX_TOKENS = 20000  # –õ–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è GigaChat

# –ö–ê–†–î–ò–ù–ê–õ–¨–ù–û –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ï –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
SEMANTIC_THRESHOLD = 0.15  # üîß –ï–©–ï –°–ù–ò–ñ–ï–ù –ø–æ—Ä–æ–≥ —Å 20% –¥–æ 15% –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
MAX_SENTENCES = 500  # üîß –£–í–ï–õ–ò–ß–ï–ù –ª–∏–º–∏—Ç —Å 200 –¥–æ 500 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
MIN_SIMILARITY_FOR_AI = 0.10  # üîß –ï–©–ï –°–ù–ò–ñ–ï–ù –ø–æ—Ä–æ–≥ –¥–ª—è AI —Å 15% –¥–æ 10%
MAX_SIMILARITY_CAP = 0.85  # üîß –ï–©–ï –£–í–ï–õ–ò–ß–ï–ù –ª–∏–º–∏—Ç —Å 75% –¥–æ 85%
CACHE_SIZE_LIMIT = 1000

# üîß –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –û–°–õ–ê–ë–õ–ï–ù–ù–´–ï –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
MAX_POSITIVE_MATCHES = 10  # –£–í–ï–õ–ò–ß–ï–ù —Å 5 –¥–æ 10 –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä
MAX_NEGATIVE_MATCHES = 5  # –£–í–ï–õ–ò–ß–ï–ù —Å 3 –¥–æ 5 –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–∞ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä
MIN_PERCENTAGE = -200  # –°–ù–ò–ñ–ï–ù —Å -100% –¥–æ -200%
MAX_PERCENTAGE = 300  # –£–í–ï–õ–ò–ß–ï–ù —Å 200% –¥–æ 300%

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —É–º–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä —Å –æ—Å–ª–∞–±–ª–µ–Ω–Ω—ã–º–∏ –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
smart_filter = SmartPhraseFilter()

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
similarity_cache = {}

def load_transcript(path: str) -> str:
    """–ß–∏—Ç–∞–µ—Ç –≤–µ—Å—å —Ç–µ–∫—Å—Ç –≤—Å—Ç—Ä–µ—á–∏ –∏–∑ .docx –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É."""
    doc = Document(path)
    return "\n".join(para.text for para in doc.paragraphs if para.text.strip())

def load_triggers(xlsx_path: str) -> dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç—Ä–∏–≥–≥–µ—Ä—ã –∏–∑ Excel —Ñ–∞–π–ª–∞ —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π –ø–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º –∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º."""
    df = pd.read_excel(xlsx_path, sheet_name='–õ–∏—Å—Ç1')
    df.columns = df.columns.str.strip()

    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∏–µ—Ä–∞—Ä—Ö–∏–∏
    df['–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è'] = df['–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è'].ffill()
    df['–ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø—Ä–æ—è–≤–ª–µ–Ω–∏—è (–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã)'] = df['–ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø—Ä–æ—è–≤–ª–µ–Ω–∏—è (–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã)'].ffill()

    result = {}
    for (comp, indicator), group in df.groupby(['–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è', '–ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø—Ä–æ—è–≤–ª–µ–Ω–∏—è (–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã)']):
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã —Å –∏—Ö –±–∞–ª–ª–∞–º–∏
        pos_markers = {}
        neg_markers = {}

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–º–∏ –º–∞—Ä–∫–µ—Ä–∞–º–∏ (10, 8, 6, 4)
        for score in [10, 8, 6, 4]:
            col_name = f'–§—Ä–∞–∑—ã/–º–∞—Ä–∫–µ—Ä—ã "–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ—è–≤–ª–µ–Ω–∏—è" {score}'
            if col_name in group.columns:
                markers = group[col_name].dropna().astype(str).str.strip()
                markers = [m for m in markers if m]
                for marker in markers:
                    pos_markers[marker] = score

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–º–∏ –º–∞—Ä–∫–µ—Ä–∞–º–∏ (10, 8, 6, 4, 2)
        for score in [10, 8, 6, 4, 2]:
            col_name = f'–§—Ä–∞–∑—ã/–º–∞—Ä–∫–µ—Ä—ã "–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ—è–≤–ª–µ–Ω–∏—è" {score}'
            if col_name in group.columns:
                markers = group[col_name].dropna().astype(str).str.strip()
                markers = [m for m in markers if m]
                for marker in markers:
                    neg_markers[marker] = score

        courses = []
        if '–∫—É—Ä—Å—ã' in group.columns and not group['–∫—É—Ä—Å—ã'].dropna().empty:
            courses = [c.strip() for c in str(group['–∫—É—Ä—Å—ã'].dropna().iloc[0]).split(',') if c.strip()]

        if comp not in result:
            result[comp] = {}

        result[comp][indicator] = {
            "positive_markers": pos_markers,
            "negative_markers": neg_markers,
            "courses": courses
        }
    logging.info(result)
    return result

def is_meaningful_phrase(phrase: str) -> bool:
    """–£–º–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ —Ñ—Ä–∞–∑—ã —Å –û–°–õ–ê–ë–õ–ï–ù–ù–´–ú–ò –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π."""
    return smart_filter.is_meaningful_phrase_basic(phrase, min_length=20, min_meaningful_words=3)

def categorize_content(text: str) -> str:
    """üîß –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–æ —Ç–µ–º–∞—Ç–∏–∫–µ"""
    text_lower = text.lower()
    
    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ/IT
    if any(word in text_lower for word in ['—Ç–µ—Ö–Ω–æ–ª–æ–≥', '—Ü–∏—Ñ—Ä–æ–≤', '–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü', 'crm', '–æ–Ω–ª–∞–π–Ω', '–ø–ª–∞—Ç—Ñ–æ—Ä–º']):
        return 'technical'
    
    # –ë–∏–∑–Ω–µ—Å/—Ñ–∏–Ω–∞–Ω—Å—ã
    if any(word in text_lower for word in ['–ª–∏–∑–∏–Ω–≥', '–∫—Ä–µ–¥–∏—Ç', '–¥–µ–ø–æ–∑–∏—Ç', '–Ω–¥—Å', '–Ω–∞–ª–æ–≥', '–±–∞–Ω–∫']):
        return 'business'
    
    # –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ–µ
    if any(word in text_lower for word in ['–∑–Ω–∞–∫–æ–º', '–ø—Ä–µ–¥—Å—Ç–∞–≤', '—Å–æ–±—Ä–∞–ª', '–≤—Å—Ç—Ä–µ—á', '–º–∞—Å—Ç–µ—Ä']):
        return 'organizational'
    
    # –§–æ—Ä–º–∞–ª—å–Ω–æ–µ –æ–±—â–µ–Ω–∏–µ
    if any(word in text_lower for word in ['—Å–ø–∞—Å–∏–±–æ', '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞', '–∏–∑–≤–∏–Ω–∏—Ç–µ', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ']):
        return 'formal'
    
    # –ú–µ–∂–ª–∏—á–Ω–æ—Å—Ç–Ω–æ–µ
    if any(word in text_lower for word in ['–∫–æ–º–∞–Ω–¥', '–∫–æ–ª–ª–µ–≥', '–æ–±—â–µ–Ω', '–ø–æ–¥–¥–µ—Ä–∂', '—Å–æ–≤–º–µ—Å—Ç–Ω']):
        return 'interpersonal'
    
    # –£–ø—Ä–∞–≤–ª–µ–Ω—á–µ—Å–∫–æ–µ
    if any(word in text_lower for word in ['—É–ø—Ä–∞–≤–ª', '–ø–ª–∞–Ω–∏—Ä', '–æ—Ä–≥–∞–Ω–∏–∑', '–∫–æ–Ω—Ç—Ä–æ–ª']):
        return 'management'
    
    return 'general'

def is_contextually_relevant(sentence: str, marker: str) -> bool:
    """üîß –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –°—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
    sent_category = categorize_content(sentence)
    marker_category = categorize_content(marker)
    
    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –º–∞—Ä–∫–µ—Ä—ã –ù–ï –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–º–∏ —Ñ—Ä–∞–∑–∞–º–∏
    if marker_category == 'technical' and sent_category == 'organizational':
        return False
    
    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –º–∞—Ä–∫–µ—Ä—ã –ù–ï –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–º –æ–±—â–µ–Ω–∏–µ–º
    if marker_category == 'technical' and sent_category == 'formal':
        return False
    
    # –ú–µ–∂–ª–∏—á–Ω–æ—Å—Ç–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã –ù–ï –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –±–∏–∑–Ω–µ—Å-–æ–±—Å—É–∂–¥–µ–Ω–∏–µ–º
    if marker_category == 'interpersonal' and sent_category == 'business':
        return False
    
    # –£–ø—Ä–∞–≤–ª–µ–Ω—á–µ—Å–∫–∏–µ –º–∞—Ä–∫–µ—Ä—ã –ù–ï –¥–æ–ª–∂–Ω—ã —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏
    if marker_category == 'management' and sent_category == 'organizational':
        return False
    
    return True

def is_contextually_relevant_positive(sentence: str, marker: str) -> bool:
    """üîß –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –û–°–õ–ê–ë–õ–ï–ù–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–ª—è –ü–û–ó–ò–¢–ò–í–ù–´–• –º–∞—Ä–∫–µ—Ä–æ–≤ - –ø–æ—á—Ç–∏ –≤—Å–µ —Ä–∞–∑—Ä–µ—à–µ–Ω–æ"""
    sent_category = categorize_content(sentence)
    marker_category = categorize_content(marker)
    
    # –î–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ —Ä–∞–∑—Ä–µ—à–∞–µ–º –ø–æ—á—Ç–∏ –≤—Å–µ
    # –ë–ª–æ–∫–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —è–≤–Ω–æ –Ω–µ–ø–æ–¥—Ö–æ–¥—è—â–∏–µ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ —Ñ—Ä–∞–∑—ã
    if len(sentence) < 30 and sent_category == 'formal':
        return False
    
    # –í—Å–µ –æ—Å—Ç–∞–ª—å–Ω–æ–µ —Ä–∞–∑—Ä–µ—à–∞–µ–º –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
    return True

def preprocess_text_optimized(text: str) -> List[str]:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    sentences = []
    current_sentence = []

    for char in text:
        current_sentence.append(char)
        if char in '.!?':
            sentence = ''.join(current_sentence).strip()
            if sentence and len(sentence) > 10:
                sentences.append(sentence)
            current_sentence = []

    if current_sentence:
        final_sentence = ''.join(current_sentence).strip()
        if final_sentence and len(final_sentence) > 10:
            sentences.append(final_sentence)

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    meaningful_sentences = []
    for sent in sentences:
        if is_meaningful_phrase(sent):
            meaningful_sentences.append(sent)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        if len(meaningful_sentences) >= MAX_SENTENCES:
            break

    return meaningful_sentences

async def check_phrase_similarity_positive(giga_chat: 'AsyncGigaChat', sent: str, marker: str, total_tokens: int) -> Tuple[float, int]:
    """üîß –°–ü–ï–¶–ò–ê–õ–¨–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ü–û–ó–ò–¢–ò–í–ù–´–• –º–∞—Ä–∫–µ—Ä–æ–≤ - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –º—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞"""
    # –ö—ç—à
    cache_key = f"POS_{hash(sent)}_{hash(marker)}"
    if cache_key in similarity_cache:
        return similarity_cache[cache_key], total_tokens

    # –î–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –º—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    if not is_contextually_relevant_positive(sent, marker):
        similarity_cache[cache_key] = 0.0
        return 0.0, total_tokens

    # –ë–∞–∑–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –±–æ–Ω—É—Å–æ–º –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö
    base_similarity = SequenceMatcher(None, sent.lower(), marker.lower()).ratio()
    
    # –î–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ —Å–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –µ—â–µ –±–æ–ª—å—à–µ
    if base_similarity < 0.05:  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∏–π –ø–æ—Ä–æ–≥
        similarity_cache[cache_key] = 0.0
        return 0.0, total_tokens
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–∫–µ–Ω—ã
    if total_tokens >= MAX_TOKENS:
        # –î–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –¥–∞–µ–º –±–æ–Ω—É—Å –∫ –±–∞–∑–æ–≤–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É
        boosted_similarity = min(base_similarity + 0.1, 0.8)
        similarity_cache[cache_key] = boosted_similarity
        return boosted_similarity, total_tokens

    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –º—è–≥–∫—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
        ai_similarity = await check_semantic_similarity_positive(giga_chat, sent, marker)
        new_tokens = total_tokens + len(sent) + len(marker)
        
        # –î–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ –±–µ—Ä–µ–º –º–∞–∫—Å–∏–º—É–º –∏–∑ –±–∞–∑–æ–≤–æ–≥–æ –∏ –ò–ò
        final_similarity = max(ai_similarity, base_similarity + 0.05)
        final_similarity = min(final_similarity, 0.8)
        
        similarity_cache[cache_key] = final_similarity
        return final_similarity, new_tokens
        
    except Exception:
        # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –¥–∞–µ–º –±–æ–Ω—É—Å –∫ –±–∞–∑–æ–≤–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É
        boosted_similarity = min(base_similarity + 0.1, 0.8)
        similarity_cache[cache_key] = boosted_similarity
        return boosted_similarity, total_tokens

async def check_phrase_similarity_optimized(giga_chat: 'AsyncGigaChat', sent: str, marker: str, total_tokens: int) -> Tuple[float, int]:
    """üîß –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –ª–æ–∂–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π"""
    # –ö—ç—à
    cache_key = f"{hash(sent)}_{hash(marker)}"
    if cache_key in similarity_cache:
        return similarity_cache[cache_key], total_tokens

    # üîß –ü–ï–†–í–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ - –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
    if not is_contextually_relevant(sent, marker):
        similarity_cache[cache_key] = 0.0
        return 0.0, total_tokens

    # –ë–∞–∑–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
    base_similarity = SequenceMatcher(None, sent.lower(), marker.lower()).ratio()
    
    # üîß –ï—Å–ª–∏ –æ—á–µ–Ω—å –Ω–∏–∑–∫–æ–µ - –æ—Ç–±—Ä–∞—Å—ã–≤–∞–µ–º
    if base_similarity < MIN_SIMILARITY_FOR_AI:
        similarity_cache[cache_key] = 0.0
        return 0.0, total_tokens
    
    # üîß –ï—Å–ª–∏ –≤—ã—Å–æ–∫–æ–µ - –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º
    if base_similarity >= MAX_SIMILARITY_CAP:
        final_similarity = min(base_similarity, MAX_SIMILARITY_CAP)
        similarity_cache[cache_key] = final_similarity
        return final_similarity, total_tokens
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–∫–µ–Ω—ã
    if total_tokens >= MAX_TOKENS:
        similarity_cache[cache_key] = base_similarity
        return base_similarity, total_tokens

    try:
        # üîß –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ AI
        ai_similarity = await check_semantic_similarity_strict(giga_chat, sent, marker)
        new_tokens = total_tokens + len(sent) + len(marker)
        
        # üîß –°–¢–†–û–ì–ê–Ø –≤–∞–ª–∏–¥–∞—Ü–∏—è AI
        if ai_similarity > MAX_SIMILARITY_CAP:
            final_similarity = min(base_similarity, MAX_SIMILARITY_CAP)
        elif ai_similarity < base_similarity * 0.6:
            final_similarity = base_similarity
        else:
            final_similarity = min(max(ai_similarity, base_similarity), MAX_SIMILARITY_CAP)
        
        similarity_cache[cache_key] = final_similarity
        return final_similarity, new_tokens
        
    except Exception:
        similarity_cache[cache_key] = base_similarity
        return base_similarity, total_tokens

async def check_semantic_similarity_positive(giga_chat: 'AsyncGigaChat', sentence: str, marker: str) -> float:
    """üîß –°–ü–ï–¶–ò–ê–õ–¨–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ü–û–ó–ò–¢–ò–í–ù–´–• –º–∞—Ä–∫–µ—Ä–æ–≤ - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –º—è–≥–∫–∞—è"""
    
    prompt = f"""–û—Ü–µ–Ω–∏ —Å–º—ã—Å–ª–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –ø–æ —à–∫–∞–ª–µ 0-1 —Å –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–´–ú –ü–û–ó–ò–¢–ò–í–ù–´–ú –£–ö–õ–û–ù–û–ú.

–§–†–ê–ó–ê –ò–ó –í–°–¢–†–ï–ß–ò: "{sentence}"
–ü–û–ó–ò–¢–ò–í–ù–´–ô –ú–ê–†–ö–ï–†: "{marker}"

–°–£–ü–ï–† –ú–Ø–ì–ö–ò–ï –ü–†–ê–í–ò–õ–ê:
- –ï—Å–ª–∏ —á–µ–ª–æ–≤–µ–∫ –≥–æ–≤–æ—Ä–∏—Ç –æ –õ–Æ–ë–´–• –∫–∞—á–µ—Å—Ç–≤–∞—Ö, –Ω–∞–≤—ã–∫–∞—Ö, —Ü–µ–ª—è—Ö = –ü–û–ó–ò–¢–ò–í–ù–û (0.5+)
- –ï—Å–ª–∏ —É–ø–æ–º–∏–Ω–∞–µ—Ç —Ä–∞–±–æ—Ç—É —Å –∫–ª–∏–µ–Ω—Ç–∞–º–∏, –∫–æ–º–∞–Ω–¥—É, —Ä–∞–∑–≤–∏—Ç–∏–µ = –ü–û–ó–ò–¢–ò–í–ù–û (0.4+)
- –ï—Å–ª–∏ –¥–µ–ª–∏—Ç—Å—è –æ–ø—ã—Ç–æ–º, –ø–ª–∞–Ω–∞–º–∏, –∑–Ω–∞–Ω–∏—è–º–∏ = –ü–û–ó–ò–¢–ò–í–ù–û (0.3+)
- –ò—â–∏ –õ–Æ–ë–´–ï —Å–º—ã—Å–ª–æ–≤—ã–µ —Å–≤—è–∑–∏, –±—É–¥—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ª–æ—è–ª—å–Ω—ã–º
- –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ 0.2, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è 0.8

–û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ 0.XX"""

    try:
        response = await giga_chat.send(prompt)
        result = float(response.strip())
        # –î–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ –ø–æ–≤—ã—à–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        return min(max(result, 0.2), 0.8)
    except:
        # –î–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ –¥–∞–µ–º –±–æ–ª—å—à–∏–π –±–∞–∑–æ–≤—ã–π –±–æ–Ω—É—Å
        base_sim = SequenceMatcher(None, sentence.lower(), marker.lower()).ratio()
        return min(max(base_sim + 0.1, 0.2), 0.8)

async def check_semantic_similarity_strict(giga_chat: 'AsyncGigaChat', sentence: str, marker: str) -> float:
    """üîß –û–°–õ–ê–ë–õ–ï–ù–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è: –ì–∏–±–∫–∞—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ AI –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π"""
    
    prompt = f"""–û—Ü–µ–Ω–∏ —Å–º—ã—Å–ª–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –ø–æ —à–∫–∞–ª–µ 0-1 —Å –ü–û–ó–ò–¢–ò–í–ù–´–ú –£–ö–õ–û–ù–û–ú.

–§–†–ê–ó–ê –ò–ó –í–°–¢–†–ï–ß–ò: "{sentence}"
–ú–ê–†–ö–ï–† –ö–û–ú–ü–ï–¢–ï–ù–¶–ò–ò: "{marker}"

–ì–ò–ë–ö–ò–ï –ü–†–ê–í–ò–õ–ê:
- –ò—â–∏ –û–ë–©–ò–ô –°–ú–´–°–õ, –∞ –Ω–µ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å–ª–æ–≤
- –ï—Å–ª–∏ —á–µ–ª–æ–≤–µ–∫ –≥–æ–≤–æ—Ä–∏—Ç –æ —Å–≤–æ–∏—Ö –∫–∞—á–µ—Å—Ç–≤–∞—Ö, —Ü–µ–ª—è—Ö, —Ä–∞–∑–≤–∏—Ç–∏–∏ - —ç—Ç–æ –ü–û–ó–ò–¢–ò–í–ù–û
- –ï—Å–ª–∏ –æ–±—Å—É–∂–¥–∞–µ—Ç —Ä–∞–±–æ—Ç—É, –∫–ª–∏–µ–Ω—Ç–æ–≤, –ø—Ä–æ—Ü–µ—Å—Å—ã - –º–æ–∂–µ—Ç –±—ã—Ç—å –ü–û–ó–ò–¢–ò–í–ù–û
- –ë—É–¥—å –ú–ï–ù–ï–ï —Å—Ç—Ä–æ–≥–∏–º, –∏—â–∏ —Å–º—ã—Å–ª–æ–≤—ã–µ —Å–≤—è–∑–∏
- –û—Ü–µ–Ω–∏–≤–∞–π –æ—Ç 0.15 –¥–æ 0.85

–û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ 0.XX"""

    try:
        response = await giga_chat.send(prompt)
        result = float(response.strip())
        return min(result, MAX_SIMILARITY_CAP)
    except:
        return SequenceMatcher(None, sentence.lower(), marker.lower()).ratio()

async def check_semantic_similarity(giga_chat: 'AsyncGigaChat', text1: str, text2: str) -> float:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–≤—É—Ö —Ñ—Ä–∞–∑ —Å –ø–æ–º–æ—â—å—é GigaChat"""
    prompt = f"""–û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Ñ—Ä–∞–∑ –ø–æ —à–∫–∞–ª–µ 0-1:

–§—Ä–∞–∑–∞ 1: "{text1}"
–§—Ä–∞–∑–∞ 2: "{text2}"

–û—Ç–≤–µ—Ç—å—Ç–µ —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–º 0.XX –±–µ–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π."""

    try:
        response = await giga_chat.send(prompt)
        return float(response.strip())
    except (ValueError, AttributeError):
        return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
    except Exception:
        return 0.0

async def analyze_text_optimized(text: str, triggers: dict, giga_chat: 'AsyncGigaChat') -> dict:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–¥—Ä–æ–±–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π"""
    start_time = time.time()
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ –∏ —Å–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    print("üîÑ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞...")
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    all_sentences = []
    current_sentence = []
    for char in text:
        current_sentence.append(char)
        if char in '.!?':
            sentence = ''.join(current_sentence).strip()
            if sentence and len(sentence) > 10:
                all_sentences.append(sentence)
            current_sentence = []
    if current_sentence:
        final_sentence = ''.join(current_sentence).strip()
        if final_sentence and len(final_sentence) > 10:
            all_sentences.append(final_sentence)
    
    # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    filter_stats = {
        'total_sentences': len(all_sentences),
        'passed_filter': 0,
        'filtered_by_pattern': 0,
        'filtered_by_length': 0,
        'filtered_by_stopwords': 0,
        'filtered_by_morphology': 0,
        'filtered_examples': {
            'pattern': [],
            'length': [],
            'stopwords': [],
            'morphology': []
        }
    }
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
    meaningful_sentences = []
    for sent in all_sentences:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        if smart_filter.is_insignificant_by_pattern(sent):
            filter_stats['filtered_by_pattern'] += 1
            if len(filter_stats['filtered_examples']['pattern']) < 5:
                filter_stats['filtered_examples']['pattern'].append(sent[:100])
            continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É (–û–°–õ–ê–ë–õ–ï–ù–ù–´–ô —Ñ–∏–ª—å—Ç—Ä)
        cleaned = smart_filter.clean_phrase(sent)
        if len(cleaned) < 20:
            filter_stats['filtered_by_length'] += 1
            if len(filter_stats['filtered_examples']['length']) < 5:
                filter_stats['filtered_examples']['length'].append(sent[:100])
            continue
        
        # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
        morph_analysis = smart_filter.analyze_morphology(sent)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ (–û–°–õ–ê–ë–õ–ï–ù–ù–´–ô —Ñ–∏–ª—å—Ç—Ä)
        if morph_analysis['total_words'] > 0:
            stopword_ratio = morph_analysis['stopwords_count'] / morph_analysis['total_words']
            if stopword_ratio > 0.85:  # –û–°–õ–ê–ë–õ–ï–ù–ù–´–ô —Å 70% –¥–æ 85%
                filter_stats['filtered_by_stopwords'] += 1
                if len(filter_stats['filtered_examples']['stopwords']) < 5:
                    filter_stats['filtered_examples']['stopwords'].append(sent[:100])
                continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞ (–û–°–õ–ê–ë–õ–ï–ù–ù–´–ô —Ñ–∏–ª—å—Ç—Ä)
        if morph_analysis['meaningful_words'] < 3:  # –û–°–õ–ê–ë–õ–ï–ù–ù–´–ô —Å 5 –¥–æ 3
            filter_stats['filtered_by_morphology'] += 1
            if len(filter_stats['filtered_examples']['morphology']) < 5:
                filter_stats['filtered_examples']['morphology'].append(sent[:100])
            continue
        
        meaningful_sentences.append(sent)
        filter_stats['passed_filter'] += 1
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        if len(meaningful_sentences) >= MAX_SENTENCES:
            break
    
    print(f"‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:")
    print(f"   –í—Å–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {filter_stats['total_sentences']}")
    print(f"   –ü—Ä–æ—à–ª–∏ —Ñ–∏–ª—å—Ç—Ä: {filter_stats['passed_filter']}")
    print(f"   –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º: {filter_stats['filtered_by_pattern']}")
    print(f"   –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ –¥–ª–∏–Ω–µ: {filter_stats['filtered_by_length']}")
    print(f"   –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞–º: {filter_stats['filtered_by_stopwords']}")
    print(f"   –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏: {filter_stats['filtered_by_morphology']}")
    
    results = {}
    total_tokens = 0
    processed_comparisons = 0
    
    # –ü–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞–∂–¥–æ–π –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏
    detailed_stats = {
        'filter_stats': filter_stats,
        'competency_stats': {}
    }

    for comp_idx, (comp, indicators) in enumerate(triggers.items()):
        comp_start = time.time()
        print(f"üîÑ –ê–Ω–∞–ª–∏–∑ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ {comp_idx+1}/{len(triggers)}: {comp}")
        
        comp_results = {
            "total_score": 0,
            "max_score": 0,
            "indicators": {},
            "indicator_scores": []  # –î–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ä–µ–¥–Ω–µ–≥–æ
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏
        comp_stats = {
            'total_markers': 0,
            'total_comparisons': 0,
            'matches_found': 0,
            'matches_below_threshold': 0,
            'contextually_irrelevant': 0,
            'marker_details': {}
        }

        for indicator, data in indicators.items():
            pos_matches = []
            neg_matches = []
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞
            indicator_stats = {
                'positive_markers': len(data['positive_markers']),
                'negative_markers': len(data['negative_markers']),
                'positive_comparisons': 0,
                'negative_comparisons': 0,
                'positive_matches': 0,
                'negative_matches': 0,
                'below_threshold': 0,
                'contextually_filtered': 0,
                'marker_analysis': {}
            }

            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–æ–∑–º–æ–∂–Ω—ã–π –±–∞–ª–ª –¥–ª—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞
            max_pos_score = max(data['positive_markers'].values(), default=0)
            max_neg_score = max(data['negative_markers'].values(), default=0)
            indicator_max_score = max_pos_score + max_neg_score
            comp_results["max_score"] += indicator_max_score

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
            for marker, score in data['positive_markers'].items():
                if not marker:
                    continue
                
                marker_stats = {
                    'marker_text': marker,
                    'score_weight': score,
                    'comparisons': 0,
                    'matches': [],
                    'below_threshold_count': 0,
                    'contextually_filtered_count': 0
                }

                for sent in meaningful_sentences:
                    similarity, total_tokens = await check_phrase_similarity_positive(
                        giga_chat, sent, marker, total_tokens
                    )
                    marker_stats['comparisons'] += 1
                    indicator_stats['positive_comparisons'] += 1
                    processed_comparisons += 1

                    if not is_contextually_relevant_positive(sent, marker):
                        marker_stats['contextually_filtered_count'] += 1
                        indicator_stats['contextually_filtered'] += 1
                        continue

                    if similarity >= SEMANTIC_THRESHOLD:
                        # üîß –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –ü–û–õ–ù–´–ô –±–∞–ª–ª –∏–∑ Excel –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
                        weighted_score = score  # –ë–µ—Ä–µ–º –ø–æ–ª–Ω—ã–π –±–∞–ª–ª –∏–∑ —Ñ–∞–π–ª–∞ —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤
                        method = "sequence_matcher" if similarity <= 0.3 else "giga_enhanced"
                        match_info = {
                            "found": sent,
                            "original": marker,
                            "score": weighted_score,
                            "similarity": similarity,
                            "method": method
                        }
                        pos_matches.append(match_info)
                        marker_stats['matches'].append(match_info)
                        indicator_stats['positive_matches'] += 1
                    elif similarity > 0.1:  # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –±–ª–∏–∑–∫–∏–µ, –Ω–æ –Ω–µ –ø—Ä–æ—à–µ–¥—à–∏–µ –ø–æ—Ä–æ–≥
                        marker_stats['below_threshold_count'] += 1
                        indicator_stats['below_threshold'] += 1

                indicator_stats['marker_analysis'][f"pos_{marker[:50]}"] = marker_stats
                comp_stats['total_markers'] += 1

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
            for marker, score in data['negative_markers'].items():
                if not marker:
                    continue
                
                marker_stats = {
                    'marker_text': marker,
                    'score_weight': score,
                    'comparisons': 0,
                    'matches': [],
                    'below_threshold_count': 0,
                    'contextually_filtered_count': 0
                }

                for sent in meaningful_sentences:
                    similarity, total_tokens = await check_phrase_similarity_optimized(
                        giga_chat, sent, marker, total_tokens
                    )
                    marker_stats['comparisons'] += 1
                    indicator_stats['negative_comparisons'] += 1
                    processed_comparisons += 1

                    if not is_contextually_relevant(sent, marker):
                        marker_stats['contextually_filtered_count'] += 1
                        indicator_stats['contextually_filtered'] += 1
                        continue

                    if similarity >= SEMANTIC_THRESHOLD:
                        # üîß –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –ü–û–õ–ù–´–ô –±–∞–ª–ª –∏–∑ Excel –¥–ª—è –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
                        weighted_score = score  # –ë–µ—Ä–µ–º –ø–æ–ª–Ω—ã–π –±–∞–ª–ª –∏–∑ —Ñ–∞–π–ª–∞ —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤
                        method = "sequence_matcher" if similarity <= 0.3 else "giga_enhanced"
                        prompt = (
                            f"–ü—Ä–µ–æ–±—Ä–∞–∑—É–π –Ω–µ–≥–∞—Ç–∏–≤–Ω—É—é —Ñ—Ä–∞–∑—É –≤ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω—É—é, –ø–æ–∑–∏—Ç–∏–≤–Ω—É—é, —Å–æ—Ö—Ä–∞–Ω—è—è —Å—É—Ç—å. "
                            f"–û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –ø–æ–∑–∏—Ç–∏–≤–Ω–æ–π —Ñ—Ä–∞–∑–æ–π –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤, —Ä–∞–∑–º–µ—Ç–∫–∏ –∏–ª–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π.\n"
                            f"–ù–µ–≥–∞—Ç–∏–≤: \"{sent}\"\n"
                            f"–ü–æ–∑–∏—Ç–∏–≤:"
                        )
                        try:
                            advice = await giga_chat.send(prompt)
                            advice = advice.strip()
                            # Post-processing: –µ—Å–ª–∏ —Å–æ–≤–µ—Ç –Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π, –ø–æ–¥—Å—Ç–∞–≤–∏—Ç—å –¥–µ—Ñ–æ–ª—Ç
                            if not advice or advice.lower().startswith("–ø–æ–∑–∏—Ç–∏–≤:") or len(advice) < 5:
                                advice = "–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ –º—ã—Å–ª—å –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ, —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ –∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º."
                        except Exception:
                            advice = "–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ –º—ã—Å–ª—å –∫–æ–Ω—Å—Ç—Ä—É–∫—Ç–∏–≤–Ω–æ, —Å –∞–∫—Ü–µ–Ω—Ç–æ–º –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ –∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º."
                        match_info = {
                            "found": sent,
                            "original": marker,
                            "score": weighted_score,
                            "similarity": similarity,
                            "method": method,
                            "advice": advice
                        }
                        neg_matches.append(match_info)
                        marker_stats['matches'].append(match_info)
                        indicator_stats['negative_matches'] += 1
                    elif similarity > 0.1:  # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –±–ª–∏–∑–∫–∏–µ, –Ω–æ –Ω–µ –ø—Ä–æ—à–µ–¥—à–∏–µ –ø–æ—Ä–æ–≥
                        marker_stats['below_threshold_count'] += 1
                        indicator_stats['below_threshold'] += 1

                indicator_stats['marker_analysis'][f"neg_{marker[:50]}"] = marker_stats
                comp_stats['total_markers'] += 1

            # –ü–æ–¥—Å—á–µ—Ç –°–†–ï–î–ù–ò–• –±–∞–ª–ª–æ–≤ –≤–º–µ—Å—Ç–æ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
            pos_score = sum(m['score'] for m in pos_matches) / len(pos_matches) if pos_matches else 0
            neg_score = sum(m['score'] for m in neg_matches) / len(neg_matches) if neg_matches else 0
            total_score = pos_score - neg_score
            
            # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–ª–ª –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ä–µ–¥–Ω–µ–≥–æ –ø–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏
            comp_results["indicator_scores"].append(total_score)

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ –í–°–ï–ú–ò –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º–∏
            comp_results["indicators"][indicator] = {
                "score": total_score,
                "max_score": indicator_max_score,
                "positive": {
                    "count": len(pos_matches),
                    "score": pos_score,
                    "examples": sorted(pos_matches, key=lambda x: x['similarity'], reverse=True)
                },
                "negative": {
                    "count": len(neg_matches),
                    "score": neg_score,
                    "examples": sorted(neg_matches, key=lambda x: x['similarity'], reverse=True)
                },
                "courses": data['courses'] if neg_score > 0 else [],  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—É—Ä—Å—ã —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ
                "detailed_stats": indicator_stats
            }
            
            comp_stats['total_comparisons'] += indicator_stats['positive_comparisons'] + indicator_stats['negative_comparisons']
            comp_stats['matches_found'] += indicator_stats['positive_matches'] + indicator_stats['negative_matches']
            comp_stats['matches_below_threshold'] += indicator_stats['below_threshold']
            comp_stats['contextually_irrelevant'] += indicator_stats['contextually_filtered']

        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª –ø–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏
        if comp_results["indicator_scores"]:
            comp_results["total_score"] = sum(comp_results["indicator_scores"]) / len(comp_results["indicator_scores"])
        else:
            comp_results["total_score"] = 0
            
        comp_time = time.time() - comp_start
        print(f"‚úÖ –ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {comp_time:.1f}—Å")
        print(f"   –ú–∞—Ä–∫–µ—Ä–æ–≤: {comp_stats['total_markers']}, –°—Ä–∞–≤–Ω–µ–Ω–∏–π: {comp_stats['total_comparisons']}")
        print(f"   –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {comp_stats['matches_found']}, –ù–∏–∂–µ –ø–æ—Ä–æ–≥–∞: {comp_stats['matches_below_threshold']}")
        print(f"   –°—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏: {comp_results['total_score']:.1f}")
        
        results[comp] = comp_results
        detailed_stats['competency_stats'][comp] = comp_stats

    total_time = time.time() - start_time
    print(f"\nüìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
    print(f"   - –í—Ä–µ–º—è: {total_time:.1f}—Å")
    print(f"   - –°—Ä–∞–≤–Ω–µ–Ω–∏–π: {processed_comparisons}")
    print(f"   - –¢–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {total_tokens}")
    print(f"   - –†–∞–∑–º–µ—Ä –∫—ç—à–∞: {len(similarity_cache)}")

    # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
    results['_detailed_stats'] = detailed_stats
    return results

def format_simple_report(analysis: dict) -> str:
    """–§–æ—Ä–º–∏—Ä—É–µ—Ç —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç —Ç–æ–ª—å–∫–æ —Å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º–∏, –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏, –±–∞–ª–ª–∞–º–∏ –∏ —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–æ–π"""
    
    report = "üéØ –ê–ù–ê–õ–ò–ó –ö–û–ú–ü–ï–¢–ï–ù–¶–ò–ô\n\n"
    report += "=" * 80 + "\n"
    
    # –ê–Ω–∞–ª–∏–∑ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π
    for comp, data in analysis.items():
        if comp == '_detailed_stats':  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            continue
        
        # –ü–æ–¥—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ –±–∞–ª–ª–∞ –ø–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏
        indicator_scores = []
        for indicator, ind_data in data["indicators"].items():
            indicator_scores.append(ind_data['score'])
        
        if indicator_scores:
            avg_score = sum(indicator_scores) / len(indicator_scores)
        else:
            avg_score = 0
        
        report += f"üèÜ {comp} - —Å—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª {avg_score:.1f}\n"
        report += "=" * 80 + "\n\n"
        
        for indicator, ind_data in data["indicators"].items():
            report += f"üìä {indicator} - {ind_data['score']:.1f} –±–∞–ª–ª–∞\n"
            report += f"   –ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö: {ind_data['positive']['score']:.1f}\n"
            report += f"   –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {ind_data['negative']['score']:.1f}\n"
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            if ind_data['positive']['examples']:
                report += "   \n   ‚úÖ –ù–ê–ô–î–ï–ù–û –í –¢–ï–ö–°–¢–ï (–ø–æ–∑–∏—Ç–∏–≤–Ω–æ–µ):\n"
                for example in ind_data['positive']['examples']:
                    report += f"      ‚Ä¢ \"{example['found'][:200]}...\"\n"
                    report += f"        –ü–æ—Ö–æ–∂–µ –Ω–∞: \"{example['original'][:100]}...\"\n"
                    report += f"        –ë–∞–ª–ª: {example['score']:.1f}\n\n"
            
            if ind_data['negative']['examples']:
                report += "   \n   ‚ùå –ù–ê–ô–î–ï–ù–û –í –¢–ï–ö–°–¢–ï (–Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–µ):\n"
                for example in ind_data['negative']['examples']:
                    report += f"      ‚Ä¢ \"{example['found'][:200]}...\"\n"
                    report += f"        –ü–æ—Ö–æ–∂–µ –Ω–∞: \"{example['original'][:100]}...\"\n"
                    report += f"        –ë–∞–ª–ª: -{example['score']:.1f}\n"
                    if 'advice' in example:
                        report += f"        –°–æ–≤–µ—Ç: {example['advice']}\n"
                    report += "\n"
            
            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫—É—Ä—Å–∞–º
            if ind_data['courses']:
                report += "   üí° –†–ï–ö–û–ú–ï–ù–î–û–í–ê–ù–ù–´–ï –ö–£–†–°–´:\n"
                for course in ind_data['courses']:
                    report += f"      ‚Ä¢ {course}\n"
                if ind_data['negative']['score'] > 0:
                    report += f"      –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: –Ω–∞–π–¥–µ–Ω –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π —Ç—Ä–∏–≥–≥–µ—Ä (–±–∞–ª–ª: -{ind_data['negative']['score']:.1f})\n"
                report += "\n"
            
            report += "-" * 70 + "\n\n"
        
        report += "\n"
    
    return report

def filter_by_main_speaker(text: str, main_speaker: str = "–ê–ª–µ–∫—Å–∞–Ω–¥—Ä") -> str:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ —Ä–µ–ø–ª–∏–∫–∏ –≥–ª–∞–≤–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞"""
    lines = text.split('\n')
    speaker_lines = []
    current_speaker = None
    
    for line in lines:
        # –ò—â–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
        match = re.search(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2} -\s*(.+?):)', line)
        if match:
            current_speaker = match.group(2).strip()
            if current_speaker == main_speaker:
                # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–ø–ª–∏–∫—É –≥–ª–∞–≤–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞ (—É–±–∏—Ä–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –º–µ—Ç–∫—É)
                speech_text = line[match.end():].strip()
                if speech_text:
                    speaker_lines.append(speech_text)
        elif current_speaker == main_speaker and line.strip():
            # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ä–µ–ø–ª–∏–∫–∏ –≥–ª–∞–≤–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
            speaker_lines.append(line.strip())
    
    return '\n'.join(speaker_lines)

class AsyncGigaChat:
    def __init__(self):
        self.session = None
        self.token = None
        self.history = []

    async def initialize(self):
        self.session = aiohttp.ClientSession()
        await self._fetch_token()

    async def close(self):
        if self.session:
            await self.session.close()

    async def _fetch_token(self):
        headers = {
            'Authorization': f'Basic {AUTH_KEY}',
            'Content-Type': 'application/x-www-form-urlencoded',
            'Accept': 'application/json',
            'RqUID': str(uuid.uuid4()),
        }
        data = {'grant_type': 'client_credentials', 'scope': SCOPE}
        async with self.session.post(API_AUTH_URL, headers=headers, data=data, ssl=False) as resp:
            js = await resp.json()
            self.token = js.get('access_token')
            if resp.status != 200 or not self.token:
                raise RuntimeError(f"Auth failed {resp.status}")

    async def send(self, prompt: str) -> str:
        if not self.token:
            await self._fetch_token()

        headers = {
            'Authorization': f'Bearer {self.token}',
            'Content-Type': 'application/json'
        }
        payload = {
            'model': 'GigaChat',
            'messages': [
                {
                    'role': 'user',
                    'content': prompt
                }
            ],
            'temperature': 0.7,
            'max_tokens': 500
        }

        async with self.session.post(API_CHAT_URL, headers=headers, json=payload, ssl=False) as resp:
            if resp.status == 200:
                js = await resp.json()
                return js['choices'][0]['message']['content']
            else:
                raise RuntimeError(f"Request failed with status {resp.status}")

async def main():
    """–ì–ª–∞–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    try:
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

        print("üîß –§–ò–ù–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø: –¢–û–õ–¨–ö–û –ì–õ–ê–í–ù–´–ô –°–ü–ò–ö–ï–† + –ü–û–õ–ù–´–ï –ë–ê–õ–õ–´")
        print("=" * 60)
        print("–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ë–ê–õ–õ–û–í:")
        print(f"   üéØ –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏: {SEMANTIC_THRESHOLD*100:.0f}% (–ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –°–ù–ò–ñ–ï–ù)")
        print(f"   üìä –õ–∏–º–∏—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {MAX_SENTENCES} (–£–í–ï–õ–ò–ß–ï–ù)")
        print(f"   üß† –û–¢–î–ï–õ–¨–ù–ê–Ø –º—è–≥–∫–∞—è –ò–ò-—Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤")
        print(f"   ‚≠ê –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø—Ä–∏—Å–≤–∞–∏–≤–∞—é—Ç—Å—è –ü–û–õ–ù–´–ï –±–∞–ª–ª—ã –∏–∑ Excel (–Ω–µ —É–º–Ω–æ–∂–µ–Ω–Ω—ã–µ)")
        print(f"   üé™ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≥–ª–∞–≤–Ω—ã–π —Å–ø–∏–∫–µ—Ä")
        print(f"   üéØ –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –±–æ–Ω—É—Å—ã –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
        print(f"   ‚öñÔ∏è –õ–∏–º–∏—Ç—ã: +{MAX_POSITIVE_MATCHES} -{MAX_NEGATIVE_MATCHES}")
        print(f"   üìà –î–∏–∞–ø–∞–∑–æ–Ω: {MIN_PERCENTAGE}% - {MAX_PERCENTAGE}%")
        print("=" * 60)
        
        logging.info("–ù–∞—á–∞–ª–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å –æ—Å–ª–∞–±–ª–µ–Ω–Ω—ã–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏")

        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤
        if not os.path.exists('./backstage/trans.docx'):
            raise FileNotFoundError("–§–∞–π–ª trans.docx –Ω–µ –Ω–∞–π–¥–µ–Ω")
        if not os.path.exists('./backstage/triggers.xlsx'):
            raise FileNotFoundError("–§–∞–π–ª triggers.xlsx –Ω–µ –Ω–∞–π–¥–µ–Ω")

        triggers = load_triggers('./backstage/triggers.xlsx')
        full_text = load_transcript('./backstage/trans.docx')
        
        text = filter_by_main_speaker(full_text, "–ê–ª–µ–∫—Å–∞–Ω–¥—Ä")
        logging.info(f"–§–∞–π–ª—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: —Ç–æ–ª—å–∫–æ —Ä–µ–ø–ª–∏–∫–∏ –≥–ª–∞–≤–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞ (–ê–ª–µ–∫—Å–∞–Ω–¥—Ä)")
        logging.info(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(full_text)} —Å–∏–º–≤–æ–ª–æ–≤, –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")

        # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GigaChat
        giga_chat = AsyncGigaChat()
        await giga_chat.initialize()

        try:
            # 3. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            logging.info("–ù–∞—á–∞–ª–æ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞...")
            analysis = await analyze_text_optimized(text, triggers, giga_chat)

            # 4. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
            final_report = format_simple_report(analysis)

            # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            with open('REPORT.txt', 'w', encoding='utf-8') as f:
                f.write(final_report)

            print(final_report)

        finally:
            await giga_chat.close()

    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞: {e}")
        raise
    finally:
        logging.info("–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")

if __name__ == '__main__':
    asyncio.run(main()) 