import pandas as pd
from docx import Document
import asyncio
import uuid
import aiohttp
import json
import re
from io import BytesIO
from typing import List, Dict, Tuple, Optional
from difflib import SequenceMatcher
import logging
import os
import string
import time

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —É–º–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä –∏ –ø–æ–¥—Ä–æ–±–Ω—É—é –æ—Ç—á—ë—Ç–Ω–æ—Å—Ç—å
from smart_filter import SmartPhraseFilter
# from detailed_report import format_detailed_report

AUTH_KEY = 'ZGMzMGJmZjEtODQwYS00ZjAwLWI2NjgtNGIyNGNiY2ViNmE1OjYwNjM3NTU0LWQxMDctNDA5ZS1hZWM3LTAwYjQ5MjZkOGU2OA=='
SCOPE = 'GIGACHAT_API_PERS'
API_AUTH_URL = 'https://ngw.devices.sberbank.ru:9443/api/v2/oauth'
API_CHAT_URL = 'https://gigachat.devices.sberbank.ru/api/v1/chat/completions'
MAX_TOKENS = 20000  # –õ–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è GigaChat

# üîß –£–ë–†–ê–ù–´ –í–°–ï –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
SEMANTIC_THRESHOLD = 0.15  # üîß –°–ù–ò–ñ–ï–ù –ø–æ—Ä–æ–≥ –¥–ª—è –ø–æ–∏—Å–∫–∞ –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
MAX_SENTENCES = 1000  # üîß –ü–û–í–´–®–ï–ù –ª–∏–º–∏—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π
MIN_SIMILARITY_FOR_AI = 0.05  # üîß –°–ù–ò–ñ–ï–ù –ø–æ—Ä–æ–≥ –¥–ª—è AI –¥–æ –º–∏–Ω–∏–º—É–º–∞
MAX_SIMILARITY_CAP = 0.95  # üîß –ü–û–í–´–®–ï–ù –ª–∏–º–∏—Ç –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
CACHE_SIZE_LIMIT = 10000

# üîß –£–ë–†–ê–ù–´ –í–°–ï –õ–ò–ú–ò–¢–´ –Ω–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
MAX_POSITIVE_MATCHES_PER_MARKER = 10  # –ü–û–í–´–®–ï–ù –¥–æ 10 —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–∞ –º–∞—Ä–∫–µ—Ä
MAX_NEGATIVE_MATCHES_PER_MARKER = 10  # –ü–û–í–´–®–ï–ù –¥–æ 10 —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–∞ –º–∞—Ä–∫–µ—Ä
MIN_PERCENTAGE = -500  # –°–ù–ò–ñ–ï–ù –¥–ª—è –ø–æ–∏—Å–∫–∞ –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
MAX_PERCENTAGE = 500  # –ü–û–í–´–®–ï–ù –¥–ª—è –ø–æ–∏—Å–∫–∞ –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —É–º–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä —Å –æ—Å–ª–∞–±–ª–µ–Ω–Ω—ã–º–∏ –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
smart_filter = SmartPhraseFilter()

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
similarity_cache = {}
# –°—á–µ—Ç—á–∏–∫ –ø–æ–ø–∞–¥–∞–Ω–∏–π –≤ –∫—ç—à –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
global_cache_hits = 0

def load_transcript(path: str) -> str:
    """–ß–∏—Ç–∞–µ—Ç –≤–µ—Å—å —Ç–µ–∫—Å—Ç –≤—Å—Ç—Ä–µ—á–∏ –∏–∑ .docx –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É."""
    doc = Document(path)
    return "\n".join(para.text for para in doc.paragraphs if para.text.strip())

def load_triggers(xlsx_path: str) -> dict:
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç—Ä–∏–≥–≥–µ—Ä—ã –∏–∑ Excel —Ñ–∞–π–ª–∞ —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π –ø–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º –∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º."""
    df = pd.read_excel(xlsx_path, sheet_name='–õ–∏—Å—Ç1')
    df.columns = df.columns.str.strip()

    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∏–µ—Ä–∞—Ä—Ö–∏–∏
    df['–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è'] = df['–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è'].ffill()
    df['–ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø—Ä–æ—è–≤–ª–µ–Ω–∏—è (–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã)'] = df['–ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø—Ä–æ—è–≤–ª–µ–Ω–∏—è (–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã)'].ffill()

    result = {}
    for (comp, indicator), group in df.groupby(['–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è', '–ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø—Ä–æ—è–≤–ª–µ–Ω–∏—è (–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã)']):
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã —Å –∏—Ö –±–∞–ª–ª–∞–º–∏
        pos_markers = {}
        neg_markers = {}

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–º–∏ –º–∞—Ä–∫–µ—Ä–∞–º–∏ (10, 8, 6, 4)
        for score in [10, 8, 6, 4]:
            col_name = f'–§—Ä–∞–∑—ã/–º–∞—Ä–∫–µ—Ä—ã "–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ—è–≤–ª–µ–Ω–∏—è" {score}'
            if col_name in group.columns:
                markers = group[col_name].dropna().astype(str).str.strip()
                markers = [m for m in markers if m]
                for marker in markers:
                    pos_markers[marker] = score

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–º–∏ –º–∞—Ä–∫–µ—Ä–∞–º–∏ (10, 8, 6, 4, 2)
        for score in [10, 8, 6, 4, 2]:
            col_name = f'–§—Ä–∞–∑—ã/–º–∞—Ä–∫–µ—Ä—ã "–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ—è–≤–ª–µ–Ω–∏—è" {score}'
            if col_name in group.columns:
                markers = group[col_name].dropna().astype(str).str.strip()
                markers = [m for m in markers if m]
                for marker in markers:
                    neg_markers[marker] = score

        courses = []
        if '–∫—É—Ä—Å—ã' in group.columns and not group['–∫—É—Ä—Å—ã'].dropna().empty:
            courses = [c.strip() for c in str(group['–∫—É—Ä—Å—ã'].dropna().iloc[0]).split(',') if c.strip()]

        if comp not in result:
            result[comp] = {}

        result[comp][indicator] = {
            "positive_markers": pos_markers,
            "negative_markers": neg_markers,
            "courses": courses
        }
    return result

def is_meaningful_phrase(phrase: str) -> bool:
    """üîß –ë–û–õ–ï–ï –°–¢–†–û–ì–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ —Ñ—Ä–∞–∑—ã –¥–ª—è –ø–æ–≤—ã—à–µ–Ω–∏—è —Ç–æ—á–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞."""
    return smart_filter.is_meaningful_phrase_basic(phrase, min_length=25, min_meaningful_words=4)

def categorize_content(text: str) -> str:
    """üîß –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø: –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–æ —Ç–µ–º–∞—Ç–∏–∫–µ"""
    text_lower = text.lower()
    
    # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ/IT
    if any(word in text_lower for word in ['—Ç–µ—Ö–Ω–æ–ª–æ–≥', '—Ü–∏—Ñ—Ä–æ–≤', '–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü', 'crm', '–æ–Ω–ª–∞–π–Ω', '–ø–ª–∞—Ç—Ñ–æ—Ä–º']):
        return 'technical'
    
    # –ë–∏–∑–Ω–µ—Å/—Ñ–∏–Ω–∞–Ω—Å—ã
    if any(word in text_lower for word in ['–ª–∏–∑–∏–Ω–≥', '–∫—Ä–µ–¥–∏—Ç', '–¥–µ–ø–æ–∑–∏—Ç', '–Ω–¥—Å', '–Ω–∞–ª–æ–≥', '–±–∞–Ω–∫']):
        return 'business'
    
    # –û—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω–æ–µ
    if any(word in text_lower for word in ['–∑–Ω–∞–∫–æ–º', '–ø—Ä–µ–¥—Å—Ç–∞–≤', '—Å–æ–±—Ä–∞–ª', '–≤—Å—Ç—Ä–µ—á', '–º–∞—Å—Ç–µ—Ä']):
        return 'organizational'
    
    # –§–æ—Ä–º–∞–ª—å–Ω–æ–µ –æ–±—â–µ–Ω–∏–µ
    if any(word in text_lower for word in ['—Å–ø–∞—Å–∏–±–æ', '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞', '–∏–∑–≤–∏–Ω–∏—Ç–µ', '–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ']):
        return 'formal'
    
    # –ú–µ–∂–ª–∏—á–Ω–æ—Å—Ç–Ω–æ–µ
    if any(word in text_lower for word in ['–∫–æ–º–∞–Ω–¥', '–∫–æ–ª–ª–µ–≥', '–æ–±—â–µ–Ω', '–ø–æ–¥–¥–µ—Ä–∂', '—Å–æ–≤–º–µ—Å—Ç–Ω']):
        return 'interpersonal'
    
    # –£–ø—Ä–∞–≤–ª–µ–Ω—á–µ—Å–∫–æ–µ
    if any(word in text_lower for word in ['—É–ø—Ä–∞–≤–ª', '–ø–ª–∞–Ω–∏—Ä', '–æ—Ä–≥–∞–Ω–∏–∑', '–∫–æ–Ω—Ç—Ä–æ–ª']):
        return 'management'
    
    return 'general'

def is_contextually_relevant(sentence: str, marker: str) -> bool:
    """üîß –£–ë–†–ê–ù–´ –í–°–ï –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π"""
    # –£–ë–†–ê–ù–´ –≤—Å–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è - –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ —Ñ—Ä–∞–∑—ã
    return True

def is_contextually_relevant_positive(sentence: str, marker: str) -> bool:
    """üîß –£–ë–†–ê–ù–´ –í–°–ï –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π"""
    # –£–ë–†–ê–ù–´ –≤—Å–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è - –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ —Ñ—Ä–∞–∑—ã
    return True

def preprocess_text_optimized(text: str) -> List[str]:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"""
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    sentences = []
    current_sentence = []

    for char in text:
        current_sentence.append(char)
        if char in '.!?':
            sentence = ''.join(current_sentence).strip()
            if sentence and len(sentence) > 10:
                sentences.append(sentence)
            current_sentence = []

    if current_sentence:
        final_sentence = ''.join(current_sentence).strip()
        if final_sentence and len(final_sentence) > 10:
            sentences.append(final_sentence)

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –∑–Ω–∞—á–∏–º—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    meaningful_sentences = []
    for sent in sentences:
        if is_meaningful_phrase(sent):
            meaningful_sentences.append(sent)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        if len(meaningful_sentences) >= MAX_SENTENCES:
            break

    return meaningful_sentences

async def check_phrase_similarity_positive(giga_chat: 'AsyncGigaChat', sent: str, marker: str, total_tokens: int) -> Tuple[float, int]:
    """üîß –°–ü–ï–¶–ò–ê–õ–¨–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ü–û–ó–ò–¢–ò–í–ù–´–• –º–∞—Ä–∫–µ—Ä–æ–≤ - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –º—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞"""
    global global_cache_hits
    # –ö—ç—à
    cache_key = f"POS_{hash(sent)}_{hash(marker)}"
    if cache_key in similarity_cache:
        global_cache_hits += 1
        return similarity_cache[cache_key], total_tokens

    # –î–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –º—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    if not is_contextually_relevant_positive(sent, marker):
        similarity_cache[cache_key] = 0.0
        return 0.0, total_tokens

    # –ë–∞–∑–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å –±–æ–Ω—É—Å–æ–º –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö
    base_similarity = SequenceMatcher(None, sent.lower(), marker.lower()).ratio()
    
    # üîß –£–ë–†–ê–ù –ø–æ—Ä–æ–≥ –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
    if base_similarity < 0.01:  # –°–ù–ò–ñ–ï–ù —Å 0.10 –¥–æ 0.01
        similarity_cache[cache_key] = 0.0
        return 0.0, total_tokens
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–∫–µ–Ω—ã
    if total_tokens >= MAX_TOKENS:
        # üîß –£–ë–†–ê–ù–´ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
        boosted_similarity = min(base_similarity + 0.1, 0.95)  # –ü–û–í–´–®–ï–ù —Å 0.05 –¥–æ 0.1, —Å 0.75 –¥–æ 0.95
        similarity_cache[cache_key] = boosted_similarity
        return boosted_similarity, total_tokens

    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –º—è–≥–∫—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
        ai_similarity = await check_semantic_similarity_positive(giga_chat, sent, marker)
        new_tokens = total_tokens + len(sent) + len(marker)
        
        # –î–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤ –±–µ—Ä–µ–º –º–∞–∫—Å–∏–º—É–º –∏–∑ –±–∞–∑–æ–≤–æ–≥–æ –∏ –ò–ò
        final_similarity = max(ai_similarity, base_similarity + 0.1)
        final_similarity = min(final_similarity, 0.95)
        
        similarity_cache[cache_key] = final_similarity
        return final_similarity, new_tokens
        
    except Exception:
        # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –¥–∞–µ–º –±–æ–Ω—É—Å –∫ –±–∞–∑–æ–≤–æ–º—É —Å—Ö–æ–¥—Å—Ç–≤—É
        boosted_similarity = min(base_similarity + 0.15, 0.95)
        similarity_cache[cache_key] = boosted_similarity
        return boosted_similarity, total_tokens

async def check_phrase_similarity_optimized(giga_chat: 'AsyncGigaChat', sent: str, marker: str, total_tokens: int) -> Tuple[float, int]:
    """üîß –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–æ–∂–µ—Å—Ç–∏ —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –ª–æ–∂–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π"""
    global global_cache_hits
    # –ö—ç—à
    cache_key = f"{hash(sent)}_{hash(marker)}"
    if cache_key in similarity_cache:
        global_cache_hits += 1
        return similarity_cache[cache_key], total_tokens

    # üîß –ü–ï–†–í–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ - –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
    if not is_contextually_relevant(sent, marker):
        similarity_cache[cache_key] = 0.0
        return 0.0, total_tokens

    # –ë–∞–∑–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
    base_similarity = SequenceMatcher(None, sent.lower(), marker.lower()).ratio()
    
    # üîß –£–ë–†–ê–ù –ø–æ—Ä–æ–≥ - –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ
    if base_similarity < 0.001:  # –°–ù–ò–ñ–ï–ù –¥–æ –º–∏–Ω–∏–º—É–º–∞
        similarity_cache[cache_key] = 0.0
        return 0.0, total_tokens
    
    # üîß –£–ë–†–ê–ù–û –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ
    if base_similarity >= 0.95:
        final_similarity = min(base_similarity, 0.95)
        similarity_cache[cache_key] = final_similarity
        return final_similarity, total_tokens
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ–∫–µ–Ω—ã
    if total_tokens >= MAX_TOKENS:
        similarity_cache[cache_key] = base_similarity
        return base_similarity, total_tokens

    try:
        # üîß –£–õ–£–ß–®–ï–ù–ù–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ AI
        ai_similarity = await check_semantic_similarity_strict(giga_chat, sent, marker)
        new_tokens = total_tokens + len(sent) + len(marker)
        
        # üîß –£–ë–†–ê–ù–ê —Å—Ç—Ä–æ–≥–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è AI
        if ai_similarity > 0.95:
            final_similarity = min(base_similarity, 0.95)
        elif ai_similarity < base_similarity * 0.3:  # –°–ù–ò–ñ–ï–ù —Å 0.6 –¥–æ 0.3
            final_similarity = base_similarity
        else:
            final_similarity = min(max(ai_similarity, base_similarity), 0.95)
        
        similarity_cache[cache_key] = final_similarity
        return final_similarity, new_tokens
        
    except Exception:
        similarity_cache[cache_key] = base_similarity
        return base_similarity, total_tokens

async def check_semantic_similarity_positive(giga_chat: 'AsyncGigaChat', sentence: str, marker: str) -> float:
    """üîß –°–ü–ï–¶–ò–ê–õ–¨–ù–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ü–û–ó–ò–¢–ò–í–ù–´–• –º–∞—Ä–∫–µ—Ä–æ–≤ - –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–∞—è"""
    
    prompt = f"""–û—Ü–µ–Ω–∏ —Å–º—ã—Å–ª–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –ø–æ —à–∫–∞–ª–µ 0-1 —Å –ë–û–õ–ï–ï –°–¢–†–û–ì–ò–ú–ò –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏.

–§–†–ê–ó–ê –ò–ó –í–°–¢–†–ï–ß–ò: "{sentence}"
–ü–û–ó–ò–¢–ò–í–ù–´–ô –ú–ê–†–ö–ï–†: "{marker}"

–°–¢–†–û–ì–ò–ï –ü–†–ê–í–ò–õ–ê:
- –ò—â–∏ –†–ï–ê–õ–¨–ù–´–ï —Å–º—ã—Å–ª–æ–≤—ã–µ —Å–≤—è–∑–∏, –∞ –Ω–µ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω—ã–µ
- –ï—Å–ª–∏ —á–µ–ª–æ–≤–µ–∫ –ö–û–ù–ö–†–ï–¢–ù–û –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–∞–≤—ã–∫ = –ü–û–ó–ò–¢–ò–í–ù–û (0.6+)
- –ï—Å–ª–∏ —É–ø–æ–º–∏–Ω–∞–µ—Ç —Ä–∞–±–æ—Ç—É —Å –∫–ª–∏–µ–Ω—Ç–∞–º–∏/–∫–æ–º–∞–Ω–¥–æ–π –ö–û–ù–ö–†–ï–¢–ù–û = –ü–û–ó–ò–¢–ò–í–ù–û (0.5+)
- –ï—Å–ª–∏ –¥–µ–ª–∏—Ç—Å—è –û–ü–´–¢–û–ú, –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ –ø–ª–∞–Ω–∞–º–∏ = –ü–û–ó–ò–¢–ò–í–ù–û (0.4+)
- –ë—É–¥—å –ë–û–õ–ï–ï –¢–†–ï–ë–û–í–ê–¢–ï–õ–¨–ù–´–ú –∫ –∫–∞—á–µ—Å—Ç–≤—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
- –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ 0.15, –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è 0.75

–û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ 0.XX"""

    try:
        response = await giga_chat.send(prompt)
        result = float(response.strip())
        # –£–ë–†–ê–ù–´ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
        return min(max(result, 0.05), 0.95)
    except:
        # –£–ë–†–ê–ù–´ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
        base_sim = SequenceMatcher(None, sentence.lower(), marker.lower()).ratio()
        return min(max(base_sim + 0.05, 0.05), 0.95)

async def check_semantic_similarity_strict(giga_chat: 'AsyncGigaChat', sentence: str, marker: str) -> float:
    """üîß –ë–û–õ–ï–ï –°–¢–†–û–ì–ê–Ø —Ñ—É–Ω–∫—Ü–∏—è: –¢—Ä–µ–±–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ AI"""
    
    prompt = f"""–û—Ü–µ–Ω–∏ —Å–º—ã—Å–ª–æ–≤–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –ø–æ —à–∫–∞–ª–µ 0-1 —Å –ë–û–õ–ï–ï –°–¢–†–û–ì–ò–ú–ò –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏.

–§–†–ê–ó–ê –ò–ó –í–°–¢–†–ï–ß–ò: "{sentence}"
–ú–ê–†–ö–ï–† –ö–û–ú–ü–ï–¢–ï–ù–¶–ò–ò: "{marker}"

–°–¢–†–û–ì–ò–ï –ü–†–ê–í–ò–õ–ê:
- –ò—â–∏ –ö–û–ù–ö–†–ï–¢–ù–´–ï —Å–º—ã—Å–ª–æ–≤—ã–µ —Å–≤—è–∑–∏, –∞ –Ω–µ –æ–±—â–∏–µ
- –ï—Å–ª–∏ —á–µ–ª–æ–≤–µ–∫ –î–ï–ú–û–ù–°–¢–†–ò–†–£–ï–¢ –Ω–∞–≤—ã–∫/–∫–∞—á–µ—Å—Ç–≤–æ - —ç—Ç–æ –ü–û–ó–ò–¢–ò–í–ù–û
- –ï—Å–ª–∏ –æ–±—Å—É–∂–¥–∞–µ—Ç –†–ï–ê–õ–¨–ù–£–Æ —Ä–∞–±–æ—Ç—É, –∞ –Ω–µ –ø–ª–∞–Ω—ã - –º–æ–∂–µ—Ç –±—ã—Ç—å –ü–û–ó–ò–¢–ò–í–ù–û
- –ë—É–¥—å –ë–û–õ–ï–ï –¢–†–ï–ë–û–í–ê–¢–ï–õ–¨–ù–´–ú –∫ –∫–∞—á–µ—Å—Ç–≤—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
- –û—Ü–µ–Ω–∏–≤–∞–π –æ—Ç 0.20 –¥–æ 0.75

–û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ 0.XX"""

    try:
        response = await giga_chat.send(prompt)
        result = float(response.strip())
        return min(result, 0.95)
    except:
        return SequenceMatcher(None, sentence.lower(), marker.lower()).ratio()

async def check_semantic_similarity(giga_chat: 'AsyncGigaChat', text1: str, text2: str) -> float:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –¥–≤—É—Ö —Ñ—Ä–∞–∑ —Å –ø–æ–º–æ—â—å—é GigaChat"""
    prompt = f"""–û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Ñ—Ä–∞–∑ –ø–æ —à–∫–∞–ª–µ 0-1:

–§—Ä–∞–∑–∞ 1: "{text1}"
–§—Ä–∞–∑–∞ 2: "{text2}"

–û—Ç–≤–µ—Ç—å—Ç–µ —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–º 0.XX –±–µ–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π."""

    try:
        response = await giga_chat.send(prompt)
        return float(response.strip())
    except (ValueError, AttributeError):
        return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
    except Exception:
        return 0.0

async def analyze_text_optimized(text: str, triggers: dict, giga_chat: 'AsyncGigaChat') -> dict:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–¥—Ä–æ–±–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π"""
    start_time = time.time()
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏ –∏ —Å–±–æ—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    print("üîÑ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞...")
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    all_sentences = []
    current_sentence = []
    for char in text:
        current_sentence.append(char)
        if char in '.!?':
            sentence = ''.join(current_sentence).strip()
            if sentence and len(sentence) > 10:
                all_sentences.append(sentence)
            current_sentence = []
    if current_sentence:
        final_sentence = ''.join(current_sentence).strip()
        if final_sentence and len(final_sentence) > 10:
            all_sentences.append(final_sentence)
    
    # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
    filter_stats = {
        'total_sentences': len(all_sentences),
        'passed_filter': 0,
        'filtered_by_pattern': 0,
        'filtered_by_length': 0,
        'filtered_by_stopwords': 0,
        'filtered_by_morphology': 0,
        'filtered_examples': {
            'pattern': [],
            'length': [],
            'stopwords': [],
            'morphology': []
        }
    }
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π
    meaningful_sentences = []
    for sent in all_sentences:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        if smart_filter.is_insignificant_by_pattern(sent):
            filter_stats['filtered_by_pattern'] += 1
            if len(filter_stats['filtered_examples']['pattern']) < 5:
                filter_stats['filtered_examples']['pattern'].append(sent[:100])
            continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É (–£–ë–†–ê–ù —Ñ–∏–ª—å—Ç—Ä)
        cleaned = smart_filter.clean_phrase(sent)
        if len(cleaned) < 5:  # –°–ù–ò–ñ–ï–ù —Å 20 –¥–æ 5
            filter_stats['filtered_by_length'] += 1
            if len(filter_stats['filtered_examples']['length']) < 5:
                filter_stats['filtered_examples']['length'].append(sent[:100])
            continue
        
        # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
        morph_analysis = smart_filter.analyze_morphology(sent)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ (–£–ë–†–ê–ù —Ñ–∏–ª—å—Ç—Ä)
        if morph_analysis['total_words'] > 0:
            stopword_ratio = morph_analysis['stopwords_count'] / morph_analysis['total_words']
            if stopword_ratio > 0.95:  # –ü–û–í–´–®–ï–ù —Å 85% –¥–æ 95%
                filter_stats['filtered_by_stopwords'] += 1
                if len(filter_stats['filtered_examples']['stopwords']) < 5:
                    filter_stats['filtered_examples']['stopwords'].append(sent[:100])
                continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞ (–£–ë–†–ê–ù —Ñ–∏–ª—å—Ç—Ä)
        if morph_analysis['meaningful_words'] < 1:  # –°–ù–ò–ñ–ï–ù —Å 3 –¥–æ 1
            filter_stats['filtered_by_morphology'] += 1
            if len(filter_stats['filtered_examples']['morphology']) < 5:
                filter_stats['filtered_examples']['morphology'].append(sent[:100])
            continue
        
        meaningful_sentences.append(sent)
        filter_stats['passed_filter'] += 1
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        if len(meaningful_sentences) >= MAX_SENTENCES:
            break
    
    print(f"‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:")
    print(f"   –í—Å–µ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {filter_stats['total_sentences']}")
    print(f"   –ü—Ä–æ—à–ª–∏ —Ñ–∏–ª—å—Ç—Ä: {filter_stats['passed_filter']}")
    print(f"   –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º: {filter_stats['filtered_by_pattern']}")
    print(f"   –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ –¥–ª–∏–Ω–µ: {filter_stats['filtered_by_length']}")
    print(f"   –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞–º: {filter_stats['filtered_by_stopwords']}")
    print(f"   –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –ø–æ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏–∏: {filter_stats['filtered_by_morphology']}")
    
    results = {}
    total_tokens = 0
    processed_comparisons = 0
    
    # –ü–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞–∂–¥–æ–π –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏
    detailed_stats = {
        'filter_stats': filter_stats,
        'competency_stats': {}
    }

    for comp_idx, (comp, indicators) in enumerate(triggers.items()):
        comp_start = time.time()
        print(f"üîÑ –ê–Ω–∞–ª–∏–∑ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ {comp_idx+1}/{len(triggers)}: {comp}")
        
        # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ö–û–ú–ü–ï–¢–ï–ù–¶–ò–ò
        total_pos_markers = 0
        total_neg_markers = 0
        print(f"üìã –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ö–û–ú–ü–ï–¢–ï–ù–¶–ò–ò '{comp}':")
        for ind_name, ind_data in indicators.items():
            if ind_name == 'courses':
                continue
            pos_count = len(ind_data.get('positive_markers', {}))
            neg_count = len(ind_data.get('negative_markers', {}))
            total_pos_markers += pos_count
            total_neg_markers += neg_count
            print(f"   üìä {ind_name}: +{pos_count} -{neg_count} –º–∞—Ä–∫–µ—Ä–æ–≤")
        print(f"   üìà –í–°–ï–ì–û: +{total_pos_markers} -{total_neg_markers} –º–∞—Ä–∫–µ—Ä–æ–≤")
        
        comp_results = {
            "total_score": 0,
            "max_score": 0,
            "indicators": {},
            "indicator_scores": []  # –î–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ä–µ–¥–Ω–µ–≥–æ
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏
        comp_stats = {
            'total_markers': 0,
            'total_comparisons': 0,
            'matches_found': 0,
            'matches_below_threshold': 0,
            'contextually_irrelevant': 0,
            'marker_details': {}
        }

        for indicator, data in indicators.items():
            pos_matches = []
            neg_matches = []
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞
            indicator_stats = {
                'positive_markers': len(data['positive_markers']),
                'negative_markers': len(data['negative_markers']),
                'positive_comparisons': 0,
                'negative_comparisons': 0,
                'positive_matches': 0,
                'negative_matches': 0,
                'below_threshold': 0,
                'contextually_filtered': 0,
                'marker_analysis': {}
            }

            # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –≤–æ–∑–º–æ–∂–Ω—ã–π –±–∞–ª–ª –¥–ª—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞
            max_pos_score = max(data['positive_markers'].values(), default=0)
            max_neg_score = max(data['negative_markers'].values(), default=0)
            indicator_max_score = max_pos_score + max_neg_score
            comp_results["max_score"] += indicator_max_score

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
            print(f"      üü¢ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {len(data['positive_markers'])} –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤...")
            for marker_idx, (marker, score) in enumerate(data['positive_markers'].items()):
                if not marker:
                    continue
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –º–∞—Ä–∫–µ—Ä–∞ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                if marker_idx < 3:
                    print(f"         +–ú–∞—Ä–∫–µ—Ä {marker_idx+1}: '{marker[:60]}...' (–±–∞–ª–ª: {score})")
                
                marker_stats = {
                    'marker_text': marker,
                    'score_weight': score,
                    'comparisons': 0,
                    'matches': [],
                    'below_threshold_count': 0,
                    'contextually_filtered_count': 0
                }

                for sent in meaningful_sentences:
                    similarity, total_tokens = await check_phrase_similarity_positive(
                        giga_chat, sent, marker, total_tokens
                    )
                    marker_stats['comparisons'] += 1
                    indicator_stats['positive_comparisons'] += 1
                    processed_comparisons += 1

                    if not is_contextually_relevant_positive(sent, marker):
                        marker_stats['contextually_filtered_count'] += 1
                        indicator_stats['contextually_filtered'] += 1
                        continue

                    if similarity >= SEMANTIC_THRESHOLD:
                        # üîß –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–∞ –º–∞—Ä–∫–µ—Ä
                        if len(marker_stats['matches']) >= MAX_POSITIVE_MATCHES_PER_MARKER:
                            continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ —É–∂–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –¥–ª—è —ç—Ç–æ–≥–æ –º–∞—Ä–∫–µ—Ä–∞
                        
                        # üîß –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –ü–û–õ–ù–´–ô –±–∞–ª–ª –∏–∑ Excel –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
                        weighted_score = score  # –ë–µ—Ä–µ–º –ø–æ–ª–Ω—ã–π –±–∞–ª–ª –∏–∑ —Ñ–∞–π–ª–∞ —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤
                        method = "sequence_matcher" if similarity <= 0.3 else "giga_enhanced"
                        match_info = {
                            "found": sent,
                            "original": marker,
                            "score": weighted_score,
                            "similarity": similarity,
                            "method": method
                        }
                        pos_matches.append(match_info)
                        marker_stats['matches'].append(match_info)
                        indicator_stats['positive_matches'] += 1
                    elif similarity > 0.1:  # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –±–ª–∏–∑–∫–∏–µ, –Ω–æ –Ω–µ –ø—Ä–æ—à–µ–¥—à–∏–µ –ø–æ—Ä–æ–≥
                        marker_stats['below_threshold_count'] += 1
                        indicator_stats['below_threshold'] += 1

                indicator_stats['marker_analysis'][f"pos_{marker[:50]}"] = marker_stats
                comp_stats['total_markers'] += 1

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
            print(f"      üî¥ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {len(data['negative_markers'])} –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤...")
            for marker_idx, (marker, score) in enumerate(data['negative_markers'].items()):
                if not marker:
                    continue
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –º–∞—Ä–∫–µ—Ä–∞ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                if marker_idx < 3:
                    print(f"         -–ú–∞—Ä–∫–µ—Ä {marker_idx+1}: '{marker[:60]}...' (–±–∞–ª–ª: {score})")
                
                marker_stats = {
                    'marker_text': marker,
                    'score_weight': score,
                    'comparisons': 0,
                    'matches': [],
                    'below_threshold_count': 0,
                    'contextually_filtered_count': 0
                }

                for sent in meaningful_sentences:
                    similarity, total_tokens = await check_phrase_similarity_optimized(
                        giga_chat, sent, marker, total_tokens
                    )
                    marker_stats['comparisons'] += 1
                    indicator_stats['negative_comparisons'] += 1
                    processed_comparisons += 1

                    if not is_contextually_relevant(sent, marker):
                        marker_stats['contextually_filtered_count'] += 1
                        indicator_stats['contextually_filtered'] += 1
                        continue

                    if similarity >= SEMANTIC_THRESHOLD:
                        # üîß –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–∞ –º–∞—Ä–∫–µ—Ä
                        if len(marker_stats['matches']) >= MAX_NEGATIVE_MATCHES_PER_MARKER:
                            continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ —É–∂–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –¥–ª—è —ç—Ç–æ–≥–æ –º–∞—Ä–∫–µ—Ä–∞
                        
                        # üîß –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø—Ä–∏—Å–≤–∞–∏–≤–∞–µ–º –ü–û–õ–ù–´–ô –±–∞–ª–ª –∏–∑ Excel –¥–ª—è –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤
                        weighted_score = score  # –ë–µ—Ä–µ–º –ø–æ–ª–Ω—ã–π –±–∞–ª–ª –∏–∑ —Ñ–∞–π–ª–∞ —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤
                        method = "sequence_matcher" if similarity <= 0.3 else "giga_enhanced"
                        match_info = {
                            "found": sent,
                            "original": marker,
                            "score": weighted_score,
                            "similarity": similarity,
                            "method": method
                        }
                        neg_matches.append(match_info)
                        marker_stats['matches'].append(match_info)
                        indicator_stats['negative_matches'] += 1
                    elif similarity > 0.1:  # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –±–ª–∏–∑–∫–∏–µ, –Ω–æ –Ω–µ –ø—Ä–æ—à–µ–¥—à–∏–µ –ø–æ—Ä–æ–≥
                        marker_stats['below_threshold_count'] += 1
                        indicator_stats['below_threshold'] += 1

                indicator_stats['marker_analysis'][f"neg_{marker[:50]}"] = marker_stats
                comp_stats['total_markers'] += 1

            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü–æ–¥—Å—á–µ—Ç –°–£–ú–ú–´ –±–∞–ª–ª–æ–≤ –∫–∞–∫ —É–∫–∞–∑–∞–Ω–æ –≤ Excel
            pos_score = sum(m['score'] for m in pos_matches) if pos_matches else 0
            neg_score = sum(m['score'] for m in neg_matches) if neg_matches else 0
            total_score = pos_score - neg_score
            
            # üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–ê –ò–ù–î–ò–ö–ê–¢–û–†–ê
            print(f"      üìä –†–ï–ó–£–õ–¨–¢–ê–¢ '{indicator}': {len(pos_matches)} –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö, {len(neg_matches)} –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
            print(f"         –ë–∞–ª–ª: +{pos_score:.1f} -{neg_score:.1f} = {total_score:.1f}")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–ª–ª –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ä–µ–¥–Ω–µ–≥–æ –ø–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏
            comp_results["indicator_scores"].append(total_score)

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ –í–°–ï–ú–ò –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è–º–∏
            comp_results["indicators"][indicator] = {
                "score": total_score,
                "max_score": indicator_max_score,
                "positive": {
                    "count": len(pos_matches),
                    "score": pos_score,
                    "examples": sorted(pos_matches, key=lambda x: x['similarity'], reverse=True)
                },
                "negative": {
                    "count": len(neg_matches),
                    "score": neg_score,
                    "examples": sorted(neg_matches, key=lambda x: x['similarity'], reverse=True)
                },
                "courses": data['courses'] if neg_score > 0 else [],  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—É—Ä—Å—ã —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ
                "detailed_stats": indicator_stats
            }
            
            comp_stats['total_comparisons'] += indicator_stats['positive_comparisons'] + indicator_stats['negative_comparisons']
            comp_stats['matches_found'] += indicator_stats['positive_matches'] + indicator_stats['negative_matches']
            comp_stats['matches_below_threshold'] += indicator_stats['below_threshold']
            comp_stats['contextually_irrelevant'] += indicator_stats['contextually_filtered']

        # –ò–°–ü–†–ê–í–õ–ï–ù–û: –°—É–º–º–∏—Ä—É–µ–º –±–∞–ª–ª—ã –ø–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏
        if comp_results["indicator_scores"]:
            comp_results["total_score"] = sum(comp_results["indicator_scores"])
        else:
            comp_results["total_score"] = 0
            
        comp_time = time.time() - comp_start
        print(f"‚úÖ –ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {comp_time:.1f}—Å")
        print(f"   –ú–∞—Ä–∫–µ—Ä–æ–≤: {comp_stats['total_markers']}, –°—Ä–∞–≤–Ω–µ–Ω–∏–π: {comp_stats['total_comparisons']}")
        print(f"   –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π: {comp_stats['matches_found']}, –ù–∏–∂–µ –ø–æ—Ä–æ–≥–∞: {comp_stats['matches_below_threshold']}")
        print(f"   –ò—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏: {comp_results['total_score']:.1f}")
        
        results[comp] = comp_results
        detailed_stats['competency_stats'][comp] = comp_stats

    total_time = time.time() - start_time
    print(f"\nüìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
    print(f"   - –í—Ä–µ–º—è: {total_time:.1f}—Å")
    print(f"   - –°—Ä–∞–≤–Ω–µ–Ω–∏–π: {processed_comparisons}")
    print(f"   - –¢–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {total_tokens}")
    print(f"   - –†–∞–∑–º–µ—Ä –∫—ç—à–∞: {len(similarity_cache)}")

    # –î–æ–±–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
    detailed_stats['similarity_cache_hits'] = global_cache_hits
    results['_detailed_stats'] = detailed_stats
    return results

def format_simple_report(analysis: dict) -> str:
    """–§–æ—Ä–º–∏—Ä—É–µ—Ç —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç —Ç–æ–ª—å–∫–æ —Å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º–∏, –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏, –±–∞–ª–ª–∞–º–∏ –∏ —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∫–æ–π"""
    
    report = "üéØ –ê–ù–ê–õ–ò–ó –ö–û–ú–ü–ï–¢–ï–ù–¶–ò–ô\n\n"
    report += "=" * 80 + "\n"
    
    # –ê–Ω–∞–ª–∏–∑ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π
    for comp, data in analysis.items():
        if comp == '_detailed_stats':  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            continue
        
        # –ü–æ–¥—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ –±–∞–ª–ª–∞ –ø–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏
        indicator_scores = []
        for indicator, ind_data in data["indicators"].items():
            indicator_scores.append(ind_data['score'])
        
        if indicator_scores:
            total_score = sum(indicator_scores)
        else:
            total_score = 0
        
        report += f"üèÜ {comp} - –∏—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª {total_score:.1f}\n"
        report += "=" * 80 + "\n\n"
        
        for indicator, ind_data in data["indicators"].items():
            report += f"üìä {indicator} - {ind_data['score']:.1f} –±–∞–ª–ª–∞\n"
            report += f"   –ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö: {ind_data['positive']['score']:.1f}\n"
            report += f"   –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {ind_data['negative']['score']:.1f}\n"
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
            if ind_data['positive']['examples']:
                report += "   \n   ‚úÖ –ù–ê–ô–î–ï–ù–û –í –¢–ï–ö–°–¢–ï (–ø–æ–∑–∏—Ç–∏–≤–Ω–æ–µ):\n"
                for example in ind_data['positive']['examples']:
                    report += f"      ‚Ä¢ \"{example['found'][:200]}...\"\n"
                    report += f"        –ü–æ—Ö–æ–∂–µ –Ω–∞: \"{example['original'][:100]}...\"\n"
                    report += f"        –ë–∞–ª–ª: {example['score']:.1f}\n\n"
            
            if ind_data['negative']['examples']:
                report += "   \n   ‚ùå –ù–ê–ô–î–ï–ù–û –í –¢–ï–ö–°–¢–ï (–Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–µ):\n"
                for example in ind_data['negative']['examples']:
                    report += f"      ‚Ä¢ \"{example['found'][:200]}...\"\n"
                    report += f"        –ü–æ—Ö–æ–∂–µ –Ω–∞: \"{example['original'][:100]}...\"\n"
                    report += f"        –ë–∞–ª–ª: -{example['score']:.1f}\n\n"
            
            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫—É—Ä—Å–∞–º
            if ind_data['courses']:
                report += "   üí° –†–ï–ö–û–ú–ï–ù–î–û–í–ê–ù–ù–´–ï –ö–£–†–°–´:\n"
                for course in ind_data['courses']:
                    report += f"      ‚Ä¢ {course}\n"
                if ind_data['negative']['score'] > 0:
                    report += f"      –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: –Ω–∞–π–¥–µ–Ω –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π —Ç—Ä–∏–≥–≥–µ—Ä (–±–∞–ª–ª: -{ind_data['negative']['score']:.1f})\n"
                report += "\n"
            
            report += "-" * 70 + "\n\n"
        
        report += "\n"
    
    return report

def filter_by_main_speaker(text: str, main_speaker: str = None) -> str:
    """–§–∏–ª—å—Ç—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ —Ä–µ–ø–ª–∏–∫–∏ –≥–ª–∞–≤–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞"""
    print(f"üéØ –§–ò–õ–¨–¢–†–ê–¶–ò–Ø –ü–û –ì–õ–ê–í–ù–û–ú–£ –°–ü–ò–ö–ï–†–£")
    
    lines = text.split('\n')
    speaker_stats = {}  # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º
    all_speakers_content = {}  # –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–æ —Å–ø–∏–∫–µ—Ä–∞–º
    current_speaker = None
    
    # –ü–µ—Ä–≤—ã–π –ø—Ä–æ—Ö–æ–¥: —Å–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å–µ–º —Å–ø–∏–∫–µ—Ä–∞–º
    for line in lines:
        match = re.search(r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2} -\s*(.+?):)', line)
        if match:
            speaker = match.group(2).strip()
            current_speaker = speaker
            
            if speaker not in speaker_stats:
                speaker_stats[speaker] = 0
                all_speakers_content[speaker] = []
            
            # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç —Ä–µ–ø–ª–∏–∫–∏
            speech_text = line[match.end():].strip()
            if speech_text:
                speaker_stats[speaker] += 1
                all_speakers_content[speaker].append(speech_text)
        elif current_speaker and line.strip():
            # –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ä–µ–ø–ª–∏–∫–∏
            speaker_stats[current_speaker] += 1
            all_speakers_content[current_speaker].append(line.strip())
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print(f"üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ü–ò–ö–ï–†–û–í:")
    sorted_speakers = sorted(speaker_stats.items(), key=lambda x: x[1], reverse=True)
    for speaker, count in sorted_speakers:
        print(f"   üé§ {speaker}: {count} —Ä–µ–ø–ª–∏–∫")
    
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ–º –≥–ª–∞–≤–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
    if main_speaker is None and sorted_speakers:
        main_speaker = sorted_speakers[0][0]  # –°–ø–∏–∫–µ—Ä —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ä–µ–ø–ª–∏–∫
        print(f"üéØ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò –í–´–ë–†–ê–ù: '{main_speaker}' ({speaker_stats[main_speaker]} —Ä–µ–ø–ª–∏–∫)")
    elif main_speaker:
        print(f"üéØ –ò–°–ü–û–õ–¨–ó–£–ï–ú –ó–ê–î–ê–ù–ù–û–ì–û: '{main_speaker}' ({speaker_stats.get(main_speaker, 0)} —Ä–µ–ø–ª–∏–∫)")
    
    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –≥–ª–∞–≤–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
    if main_speaker in all_speakers_content:
        result = '\n'.join(all_speakers_content[main_speaker])
        print(f"‚úÖ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {len(result)} —Å–∏–º–≤–æ–ª–æ–≤ –æ—Ç —Å–ø–∏–∫–µ—Ä–∞ '{main_speaker}'")
        return result
    else:
        print(f"‚ùå –°–ø–∏–∫–µ—Ä '{main_speaker}' –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return ""

class AsyncGigaChat:
    def __init__(self):
        self.session = None
        self.token = None
        self.history = []
        self.total_requests = 0  # –°—á–µ—Ç—á–∏–∫ –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ API

    async def initialize(self):
        self.session = aiohttp.ClientSession()
        await self._fetch_token()

    async def close(self):
        if self.session:
            await self.session.close()

    async def _fetch_token(self):
        headers = {
            'Authorization': f'Basic {AUTH_KEY}',
            'Content-Type': 'application/x-www-form-urlencoded',
            'Accept': 'application/json',
            'RqUID': str(uuid.uuid4()),
        }
        data = {'grant_type': 'client_credentials', 'scope': SCOPE}
        async with self.session.post(API_AUTH_URL, headers=headers, data=data, ssl=False) as resp:
            js = await resp.json()
            self.token = js.get('access_token')
            if resp.status != 200 or not self.token:
                raise RuntimeError(f"Auth failed {resp.status}")

    async def send(self, prompt: str) -> str:
        if not self.token:
            await self._fetch_token()

        self.history.append({'role': 'user', 'content': prompt})
        headers = {
            'Authorization': f'Bearer {self.token}',
            'Content-Type': 'application/json'
        }
        payload = {
            'model': 'GigaChat',
            'messages': self.history,
            'temperature': 0.7,
            'max_tokens': 500
        }

        self.total_requests += 1  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
        async with self.session.post(API_CHAT_URL, headers=headers, json=payload, ssl=False) as resp:
            if resp.status == 200:
                js = await resp.json()
                return js['choices'][0]['message']['content']
            else:
                raise RuntimeError(f"Request failed with status {resp.status}")

async def main():
    """–ì–ª–∞–≤–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    try:
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

        print("üîß –§–ò–ù–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø: –¢–û–õ–¨–ö–û –ì–õ–ê–í–ù–´–ô –°–ü–ò–ö–ï–† + –ü–û–õ–ù–´–ï –ë–ê–õ–õ–´")
        print("=" * 60)
        print("–ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ë–ê–õ–õ–û–í:")
        print(f"   üéØ –ü–æ—Ä–æ–≥ —Å—Ö–æ–∂–µ—Å—Ç–∏: {SEMANTIC_THRESHOLD*100:.0f}% (–í–´–°–û–ö–ò–ô –î–õ–Ø –¢–û–ß–ù–û–°–¢–ò)")
        print(f"   üìä –õ–∏–º–∏—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π: {MAX_SENTENCES} (–£–í–ï–õ–ò–ß–ï–ù)")
        print(f"   üß† –û–¢–î–ï–õ–¨–ù–ê–Ø –º—è–≥–∫–∞—è –ò–ò-—Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö –º–∞—Ä–∫–µ—Ä–æ–≤")
        print(f"   ‚≠ê –ò–°–ü–†–ê–í–õ–ï–ù–û: –ø—Ä–∏—Å–≤–∞–∏–≤–∞—é—Ç—Å—è –ü–û–õ–ù–´–ï –±–∞–ª–ª—ã –∏–∑ Excel (–Ω–µ —É–º–Ω–æ–∂–µ–Ω–Ω—ã–µ)")
        print(f"   üé™ –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –≥–ª–∞–≤–Ω—ã–π —Å–ø–∏–∫–µ—Ä")
        print(f"   üéØ –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –±–æ–Ω—É—Å—ã –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π")
        print(f"   ‚öñÔ∏è –õ–∏–º–∏—Ç—ã: –ú–∞–∫—Å–∏–º—É–º {MAX_POSITIVE_MATCHES_PER_MARKER} —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –Ω–∞ –º–∞—Ä–∫–µ—Ä")
        print(f"   üìà –î–∏–∞–ø–∞–∑–æ–Ω: {MIN_PERCENTAGE}% - {MAX_PERCENTAGE}%")
        print("=" * 60)
        
        logging.info("–ù–∞—á–∞–ª–æ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å –æ—Å–ª–∞–±–ª–µ–Ω–Ω—ã–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏")

        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–æ–≤
        if not os.path.exists('./trans.docx'):
            raise FileNotFoundError("–§–∞–π–ª trans.docx –Ω–µ –Ω–∞–π–¥–µ–Ω")
        if not os.path.exists('./triggers.xlsx'):
            raise FileNotFoundError("–§–∞–π–ª triggers.xlsx –Ω–µ –Ω–∞–π–¥–µ–Ω")

        triggers = load_triggers('./triggers.xlsx')
        full_text = load_transcript('./trans.docx')
        
        text = filter_by_main_speaker(full_text)  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ–º –≥–ª–∞–≤–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
        logging.info(f"–§–∞–π–ª—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è: —Ç–æ–ª—å–∫–æ —Ä–µ–ø–ª–∏–∫–∏ –≥–ª–∞–≤–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞ (–ê–ª–µ–∫—Å–∞–Ω–¥—Ä)")
        logging.info(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {len(full_text)} —Å–∏–º–≤–æ–ª–æ–≤, –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")

        # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GigaChat
        giga_chat = AsyncGigaChat()
        await giga_chat.initialize()

        try:
            # 3. –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            logging.info("–ù–∞—á–∞–ª–æ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—Å—Ç–∞...")
            analysis = await analyze_text_optimized(text, triggers, giga_chat)

            # 4. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞
            final_report = format_simple_report(analysis)

            # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            with open('REPORT.txt', 'w', encoding='utf-8') as f:
                f.write(final_report)

            print(final_report)

        finally:
            await giga_chat.close()

    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞: {e}")
        raise
    finally:
        logging.info("–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")

if __name__ == '__main__':
    asyncio.run(main()) 