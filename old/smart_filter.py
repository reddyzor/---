import nltk
import re
import string
from typing import List, Set
import asyncio

class SmartPhraseFilter:
    def __init__(self):
        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä—É—Å—Å–∫–∏—Ö —Å—Ç–æ–ø-—Å–ª–æ–≤
        self.extended_stopwords = {
            '–∏', '–≤', '–≤–æ', '–Ω–µ', '—á—Ç–æ', '–æ–Ω', '–Ω–∞', '—è', '—Å', '—Å–æ', '–∫–∞–∫', '–∞', '—Ç–æ', '–≤—Å–µ', '–æ–Ω–∞', '—Ç–∞–∫',
            '–µ–≥–æ', '–Ω–æ', '–¥–∞', '—Ç—ã', '–∫', '—É', '–∂–µ', '–≤—ã', '–∑–∞', '–±—ã', '–ø–æ', '—Ç–æ–ª—å–∫–æ', '–µ–µ', '–º–Ω–µ', '–±—ã–ª–æ',
            '–≤–æ—Ç', '–æ—Ç', '–º–µ–Ω—è', '–µ—â–µ', '–Ω–µ—Ç', '–æ', '–∏–∑', '–µ–º—É', '—Ç–µ–ø–µ—Ä—å', '–∫–æ–≥–¥–∞', '–¥–∞–∂–µ', '–Ω—É', '–≤–¥—Ä—É–≥',
            '–ª–∏', '–µ—Å–ª–∏', '—É–∂–µ', '–∏–ª–∏', '–Ω–∏', '–±—ã—Ç—å', '–±—ã–ª', '–Ω–µ–≥–æ', '–¥–æ', '–≤–∞—Å', '–Ω–∏–±—É–¥—å', '–æ–ø—è—Ç—å', '—É–∂',
            '–≤–∞–º', '–≤–µ–¥—å', '—Ç–∞–º', '–ø–æ—Ç–æ–º', '—Å–µ–±—è', '–Ω–∏—á–µ–≥–æ', '–µ–π', '–º–æ–∂–µ—Ç', '–æ–Ω–∏', '—Ç—É—Ç', '–≥–¥–µ', '–µ—Å—Ç—å',
            '–Ω–∞–¥–æ', '–Ω–µ–π', '–¥–ª—è', '–º—ã', '—Ç–µ–±—è', '–∏—Ö', '—á–µ–º', '–±—ã–ª–∞', '—Å–∞–º', '—á—Ç–æ–±', '–±–µ–∑', '–±—É–¥—Ç–æ', '—á–µ–≥–æ',
            '—Ä–∞–∑', '—Ç–æ–∂–µ', '—Å–µ–±–µ', '–ø–æ–¥', '–±—É–¥–µ—Ç', '–∂', '—Ç–æ–≥–¥–∞', '–∫—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ–≥–æ', '–ø–æ—Ç–æ–º—É', '—ç—Ç–æ–≥–æ',
            '–∫–∞–∫–æ–π', '—Å–æ–≤—Å–µ–º', '–Ω–∏–º', '–∑–¥–µ—Å—å', '—ç—Ç–æ–º', '–æ–¥–∏–Ω', '–ø–æ—á—Ç–∏', '–º–æ–π', '—Ç–µ–º', '—á—Ç–æ–±—ã', '–Ω–µ–µ',
            '—Å–µ–π—á–∞—Å', '–±—ã–ª–∏', '–∫—É–¥–∞', '–∑–∞—á–µ–º', '–≤—Å–µ—Ö', '–Ω–∏–∫–æ–≥–¥–∞', '–º–æ–∂–Ω–æ', '–ø—Ä–∏', '–Ω–∞–∫–æ–Ω–µ—Ü', '–¥–≤–∞',
            '–æ–±', '–¥—Ä—É–≥–æ–π', '—Ö–æ—Ç—å', '–ø–æ—Å–ª–µ', '–Ω–∞–¥', '–±–æ–ª—å—à–µ', '—Ç–æ—Ç', '—á–µ—Ä–µ–∑', '—ç—Ç–∏', '–Ω–∞—Å', '–ø—Ä–æ', '–≤—Å–µ–≥–æ',
            '–Ω–∏—Ö', '–∫–∞–∫–∞—è', '–º–Ω–æ–≥–æ', '—Ä–∞–∑–≤–µ', '—Ç—Ä–∏', '—ç—Ç—É', '–º–æ—è', '–≤–ø—Ä–æ—á–µ–º', '—Ö–æ—Ä–æ—à–æ', '—Å–≤–æ—é', '—ç—Ç–æ–π',
            '–ø–µ—Ä–µ–¥', '–∏–Ω–æ–≥–¥–∞', '–ª—É—á—à–µ', '—á—É—Ç—å', '—Ç–æ–º', '–Ω–µ–ª—å–∑—è', '—Ç–∞–∫–æ–π', '–∏–º', '–±–æ–ª–µ–µ', '–≤—Å–µ–≥–¥–∞',
            '–∫–æ–Ω–µ—á–Ω–æ', '–≤—Å—é', '–º–µ–∂–¥—É', '—ç—Ç–æ', '–≤—Å—ë', '—Ç–æ', '—ç—Ç–æ—Ç', '—Ç–æ—Ç', '—ç—Ç–∞', '—ç—Ç–∏', '—Ç–µ', '–º–æ–∏',
            '—Ç–≤–æ–∏', '–µ–≥–æ', '–µ—ë', '–∏—Ö', '–Ω–µ–≥–æ', '–Ω–µ—ë', '–Ω–∏—Ö', '–º–Ω–µ', '–º–Ω–æ–π', '–º–Ω–æ—é', '—Ç–µ–±–µ', '—Ç–µ–±—è',
            '—Ç–æ–±–æ–π', '—Ç–æ–±–æ—é', '–Ω–∞–º', '–Ω–∞–º–∏', '–≤–∞–º', '–≤–∞–º–∏', '–∏–º', '–∏–º–∏', '—ç—Ç–æ', '—ç—Ç–æ—Ç', '—ç—Ç–∞', '—ç—Ç–∏',
            '—Ç–æ—Ç', '—Ç–∞', '—Ç–µ', '—Ç–∞–∫–æ–π', '—Ç–∞–∫–∞—è', '—Ç–∞–∫–∏–µ', '—Ç–∞–∫–æ–µ', '—Ç–∞–∫', '—Ç–∞–∫–∂–µ', '—Ç–æ–∂–µ', '–ª–∏–±–æ',
            '–∏–ª–∏', '–Ω–∏', '–Ω–µ', '–Ω–µ—Ç', '–Ω–∏—á–µ–≥–æ', '–Ω–∏–∫—Ç–æ', '–Ω–∏–∫–æ–≥–¥–∞', '–Ω–∏–≥–¥–µ', '–Ω–∏–∫—É–¥–∞', '–Ω–∏–æ—Ç–∫—É–¥–∞',
            '–Ω–∏–∫–∞–∫', '–Ω–∏—Å–∫–æ–ª—å–∫–æ', '–Ω–∏—á—É—Ç—å', '–Ω–∏—á—É—Ç—å', '–Ω–∏—á—É—Ç—å', '–Ω–∏—á—É—Ç—å', '–Ω–∏—á—É—Ç—å', '–Ω–∏—á—É—Ç—å', '–Ω–∏—á—É—Ç—å'
        }
        
        self.setup_nltk()
        self.russian_stopwords = set()
        self.load_russian_stopwords()
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–µ–∑–Ω–∞—á–∏–º—ã—Ö —Ñ—Ä–∞–∑ (–£–ë–†–ê–ù–´ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π)
        self.insignificant_patterns = [
            r'^(–¥–∞|–Ω–µ—Ç|–∞–≥–∞|—É–≥—É|–Ω—É|—Ö–º|—ç–º|—ç)\s*[,.!?]*$',  # –ú–µ–∂–¥–æ–º–µ—Ç–∏—è
            r'^\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}.*?:.*$',  # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
            # –£–ë–†–ê–ù–´ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –±–æ–ª–µ–µ –º—è–≥–∫–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        ]
    
    def setup_nltk(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ NLTK"""
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            nltk.data.find('corpora/stopwords')
        except LookupError:
            print("–°–∫–∞—á–∏–≤–∞—é —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ NLTK...")
            nltk.download('stopwords', quiet=True)
    
    def load_russian_stopwords(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä—É—Å—Å–∫–∏–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –∏–∑ NLTK"""
        try:
            from nltk.corpus import stopwords
            self.russian_stopwords = set(stopwords.words('russian'))
            # –î–æ–±–∞–≤–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            self.russian_stopwords.update(self.extended_stopwords)
            
        except Exception as e:
            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ NLTK: {e}")
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—ã–π –Ω–∞–±–æ—Ä —Ä—É—Å—Å–∫–∏—Ö —Å—Ç–æ–ø-—Å–ª–æ–≤ –∫–∞–∫ fallback
            self.russian_stopwords = self.extended_stopwords
    
    def clean_phrase(self, phrase: str) -> str:
        """–û—á–∏—â–∞–µ—Ç —Ñ—Ä–∞–∑—É –æ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –∏ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        # –£–±–∏—Ä–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        cleaned = re.sub(r'\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2} -\s*\w+:\s*', '', phrase)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        cleaned = re.sub(r'\s+', ' ', cleaned).strip()
        
        return cleaned
    
    def is_insignificant_by_pattern(self, phrase: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ—Ä–∞–∑–∞ –Ω–µ–∑–Ω–∞—á–∏–º–æ–π –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º"""
        cleaned_phrase = self.clean_phrase(phrase).lower()
        
        for pattern in self.insignificant_patterns:
            if re.match(pattern, cleaned_phrase, re.IGNORECASE):
                return True
        return False
    
    def analyze_morphology(self, phrase: str) -> dict:
        """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ñ—Ä–∞–∑—ã –±–µ–∑ pymorphy2"""
        cleaned_phrase = self.clean_phrase(phrase)
        words = cleaned_phrase.translate(str.maketrans('', '', string.punctuation)).split()
        
        if not words:
            return {
                'total_words': 0,
                'meaningful_words': 0,
                'stopwords_count': 0,
                'interjections_count': 0,
                'significant_pos_count': 0
            }
        
        meaningful_words = []
        stopwords_count = 0
        interjections_count = 0
        significant_pos_count = 0
        
        # –ü—Ä–æ—Å—Ç—ã–µ –º–µ–∂–¥–æ–º–µ—Ç–∏—è –∏ –Ω–µ–∑–Ω–∞—á–∏–º—ã–µ —Å–ª–æ–≤–∞
        interjections = {
            '—É–≥—É', '–∞–≥–∞', '—Ö–º', '—ç–º', '—ç', '–Ω—É', '–¥–∞', '–Ω–µ—Ç', '—Ö–æ—Ä–æ—à–æ', '–ø–æ–Ω—è—Ç–Ω–æ', 
            '—è—Å–Ω–æ', '–ª–∞–¥–Ω–æ', '—Ç–æ—á–Ω–æ', '–∫–æ–Ω–µ—á–Ω–æ', '—Ä–∞–∑—É–º–µ–µ—Ç—Å—è', '—Å–ø–∞—Å–∏–±–æ', '–ø–æ–∂–∞–ª—É–π—Å—Ç–∞'
        }
        
        for word in words:
            word_lower = word.lower()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
            if word_lower in self.russian_stopwords:
                stopwords_count += 1
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–µ–∂–¥–æ–º–µ—Ç–∏—è
            if word_lower in interjections:
                interjections_count += 1
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–ª–∏–Ω—É —Å–ª–æ–≤–∞ (—Å–ª–æ–≤–∞ –¥–ª–∏–Ω–Ω–µ–µ 2 —Å–∏–º–≤–æ–ª–æ–≤ —Å—á–∏—Ç–∞—é—Ç—Å—è –∑–Ω–∞—á–∏–º—ã–º–∏)
            if len(word_lower) > 2:
                significant_pos_count += 1
                meaningful_words.append(word_lower)
        
        return {
            'total_words': len(words),
            'meaningful_words': len(meaningful_words),
            'stopwords_count': stopwords_count,
            'interjections_count': interjections_count,
            'significant_pos_count': significant_pos_count,
            'meaningful_word_list': meaningful_words
        }
    
    def is_meaningful_phrase_basic(self, phrase: str, min_length: int = 5, min_meaningful_words: int = 1) -> bool:
        """üîß –£–ë–†–ê–ù–´ –í–°–ï –û–ì–†–ê–ù–ò–ß–ï–ù–ò–Ø –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º (—Ç–æ–ª—å–∫–æ —Å–∞–º—ã–µ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ)
        if self.is_insignificant_by_pattern(phrase):
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É (–°–ù–ò–ñ–ï–ù–ê)
        cleaned = self.clean_phrase(phrase)
        if len(cleaned) < min_length:
            return False
        
        # –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
        morph_analysis = self.analyze_morphology(phrase)
        
        # üîß –£–ë–†–ê–ù–ê –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–æ–ø-—Å–ª–æ–≤
        # if morph_analysis['total_words'] > 0:
        #     stopword_ratio = morph_analysis['stopwords_count'] / morph_analysis['total_words']
        #     if stopword_ratio > 0.95:  # –ü–û–í–´–®–ï–ù —Å 60% –¥–æ 95%
        #         return False
        
        # üîß –£–ë–†–ê–ù–´ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∑–Ω–∞—á–∏–º—ã–º —Å–ª–æ–≤–∞–º
        if morph_analysis['meaningful_words'] >= min_meaningful_words:
            return True
        
        # üîß –£–ë–†–ê–ù–´ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –∑–Ω–∞—á–∏–º—ã–º —á–∞—Å—Ç—è–º —Ä–µ—á–∏
        if morph_analysis['significant_pos_count'] >= min_meaningful_words:
            return True
        
        return False
    
    async def is_meaningful_phrase_ai(self, phrase: str, giga_chat, context: str = "–∞–Ω–∞–ª–∏–∑ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π") -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ —Ñ—Ä–∞–∑—ã —Å –ø–æ–º–æ—â—å—é –ò–ò"""
        # –°–Ω–∞—á–∞–ª–∞ –±–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        if not self.is_meaningful_phrase_basic(phrase):
            return False
        
        # –ï—Å–ª–∏ –ø—Ä–æ—à–ª–∞ –±–∞–∑–æ–≤—É—é –ø—Ä–æ–≤–µ—Ä–∫—É, –Ω–æ –≤—Å–µ –µ—â–µ —Å–æ–º–Ω–µ–≤–∞–µ–º—Å—è, —Å–ø—Ä–∞—à–∏–≤–∞–µ–º –ò–ò
        cleaned_phrase = self.clean_phrase(phrase)
        
        prompt = f"""–û–ø—Ä–µ–¥–µ–ª–∏, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –¥–∞–Ω–Ω–∞—è —Ñ—Ä–∞–∑–∞ –∑–Ω–∞—á–∏–º—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è {context}.

–§—Ä–∞–∑–∞: "{cleaned_phrase}"

–ö—Ä–∏—Ç–µ—Ä–∏–∏ –Ω–µ–∑–Ω–∞—á–∏–º—ã—Ö —Ñ—Ä–∞–∑:
- –ú–µ–∂–¥–æ–º–µ—Ç–∏—è (—É–≥—É, –∞–≥–∞, —Ö–º)
- –§–æ—Ä–º–∞–ª—å–Ω—ã–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è/–ø—Ä–æ—â–∞–Ω–∏—è
- –ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è (–º–µ–Ω—è –∑–æ–≤—É—Ç...)
- –ß–∏—Å—Ç–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä–µ–ø–ª–∏–∫–∏
- –ü–µ—Ä–µ—Å–ø—Ä–æ—Å—ã –±–µ–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è

–û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ: –î–ê (–∑–Ω–∞—á–∏–º–∞—è) –∏–ª–∏ –ù–ï–¢ (–Ω–µ–∑–Ω–∞—á–∏–º–∞—è)"""

        try:
            response = await giga_chat.send(prompt)
            return "–¥–∞" in response.lower()
        except Exception:
            # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –ø—Ä–æ–≤–µ—Ä–∫—É
            return True

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç
def create_smart_filter():
    """–°–æ–∑–¥–∞–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —É–º–Ω—ã–π —Ñ–∏–ª—å—Ç—Ä"""
    return SmartPhraseFilter()

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
if __name__ == "__main__":
    filter = SmartPhraseFilter()
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ —Ñ—Ä–∞–∑—ã
    test_phrases = [
        "–£–≥—É, –ø–æ–Ω—è—Ç–Ω–æ",
        "–ú–µ–Ω—è –∑–æ–≤—É—Ç –ò–≤–∞–Ω",
        "–Ø —Ä–∞–±–æ—Ç–∞—é –≤ –±–∞–Ω–∫–µ —É–∂–µ 5 –ª–µ—Ç –∏ –∑–∞–Ω–∏–º–∞—é—Å—å –∫—Ä–µ–¥–∏—Ç–æ–≤–∞–Ω–∏–µ–º –º–∞–ª–æ–≥–æ –±–∏–∑–Ω–µ—Å–∞",
        "–°–ø–∞—Å–∏–±–æ –∑–∞ –≤—Å—Ç—Ä–µ—á—É",
        "–ú—ã –≤–Ω–µ–¥—Ä–∏–ª–∏ –Ω–æ–≤—É—é CRM —Å–∏—Å—Ç–µ–º—É –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤",
        "–•–º, –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ",
        "–Ø —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å –æ—Ç–¥–µ–ª–∞ –∏ –æ—Ç–≤–µ—á–∞—é –∑–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∫–æ–Ω—Ç—Ä–æ–ª—å —Ä–∞–±–æ—Ç—ã –∫–æ–º–∞–Ω–¥—ã"
    ]
    
    print("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É–º–Ω–æ–≥–æ —Ñ–∏–ª—å—Ç—Ä–∞:")
    for phrase in test_phrases:
        is_meaningful = filter.is_meaningful_phrase_basic(phrase)
        print(f"'{phrase}' -> {'–ó–ù–ê–ß–ò–ú–ê–Ø' if is_meaningful else '–ù–ï–ó–ù–ê–ß–ò–ú–ê–Ø'}") 