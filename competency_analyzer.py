import asyncio
import logging
import os
from typing import Dict, Tuple
import pandas as pd
from docx import Document
import aiohttp
import json
import uuid
from datetime import datetime
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from search_optimized import AsyncGigaChat, load_triggers, load_transcript, filter_by_main_speaker, analyze_text_optimized, format_simple_report

class CompetencyAnalyzer:
    """–ö–ª–∞—Å—Å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –≤ Telegram –±–æ—Ç"""
    
    def __init__(self):
        self.giga_chat = None
        
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GigaChat"""
        self.giga_chat = AsyncGigaChat()
        await self.giga_chat.initialize()
        
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        if self.giga_chat:
            await self.giga_chat.close()
            
    async def analyze_competencies(self, trans_file_path: str, triggers_file_path: str) -> Tuple[str, str]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç –∏ –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ
        
        Args:
            trans_file_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Ç–µ–∫—Å—Ç–æ–º –≤—Å—Ç—Ä–µ—á–∏
            triggers_file_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Ç—Ä–∏–≥–≥–µ—Ä–∞–º–∏
            
        Returns:
            Tuple[str, str]: (–ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç, –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ)
        """
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª—ã
            triggers = load_triggers(triggers_file_path)
            full_text = load_transcript(trans_file_path)
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –≥–ª–∞–≤–Ω–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
            text = filter_by_main_speaker(full_text, "–ê–ª–µ–∫—Å–∞–Ω–¥—Ä")
            
            logging.info(f"–ê–Ω–∞–ª–∏–∑ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π: {len(full_text)} -> {len(text)} —Å–∏–º–≤–æ–ª–æ–≤")
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç
            analysis = await analyze_text_optimized(text, triggers, self.giga_chat)
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç (–∫–∞–∫ –≤ REPORT.txt)
            full_report = self._create_detailed_report(analysis, trans_file_path, triggers_file_path)
            
            # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ (–æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å)
            summary = self._create_summary(analysis)
            
            return full_report, summary
            
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π: {e}")
            raise
            
    def _create_summary(self, analysis: Dict) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –ø–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º"""
        summary = "üìä –ö–†–ê–¢–ö–û–ï –†–ï–ó–Æ–ú–ï –ü–û –ö–û–ú–ü–ï–¢–ï–ù–¶–ò–Ø–ú\n\n"
        
        competency_scores = {}
        
        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏—è–º
        for comp, data in analysis.items():
            if comp == '_detailed_stats':
                continue
                
            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª –ø–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏
            indicator_scores = []
            courses_for_comp = set()
            
            for indicator, ind_data in data["indicators"].items():
                indicator_scores.append(ind_data['score'])
                if ind_data['courses']:
                    courses_for_comp.update(ind_data['courses'])
            
            if indicator_scores:
                avg_score = sum(indicator_scores) / len(indicator_scores)
                competency_scores[comp] = {
                    'score': avg_score,
                    'courses': list(courses_for_comp),
                    'has_negative': any(ind_data['negative']['score'] > 0 for ind_data in data["indicators"].values())
                }
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏ –ø–æ –±–∞–ª–ª–∞–º
        sorted_competencies = sorted(competency_scores.items(), key=lambda x: x[1]['score'])
        
        for comp, data in sorted_competencies:
            score = data['score']
            courses = data['courses']
            has_negative = data['has_negative']
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏
            if score >= 8:
                status = "‚úÖ –û—Ç–ª–∏—á–Ω–æ —Ä–∞–∑–≤–∏—Ç–∞"
                emoji = "üü¢"
            elif score >= 6:
                status = "üü° –¢—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è"
                emoji = "üü°"
            else:
                status = "‚ùå –¢—Ä–µ–±—É–µ—Ç —Ä–∞–∑–≤–∏—Ç–∏—è"
                emoji = "üî¥"
            
            summary += f"{emoji} **{comp}** - {score:.1f}/10 –±–∞–ª–ª–æ–≤\n"
            summary += f"   {status}\n"
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—É—Ä—Å—ã —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –ø—Ä–æ—è–≤–ª–µ–Ω–∏—è
            if has_negative and courses:
                summary += f"   üìö **–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∫—É—Ä—Å—ã:**\n"
                for course in courses[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–∞–∫—Å–∏–º—É–º 3 –∫—É—Ä—Å–∞
                    summary += f"      ‚Ä¢ {course}\n"
                summary += "\n"
            elif score < 6 and courses:
                summary += f"   üìö **–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –∫—É—Ä—Å—ã:**\n"
                for course in courses[:3]:
                    summary += f"      ‚Ä¢ {course}\n"
                summary += "\n"
            else:
                summary += "\n"
        
        # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        low_score_comps = [comp for comp, data in competency_scores.items() if data['score'] < 6]
        if low_score_comps:
            summary += "üí° **–û–ë–©–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:**\n"
            summary += f"   ‚Ä¢ –û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Ä–∞–∑–≤–∏—Ç–∏–µ: {', '.join(low_score_comps[:3])}\n"
            summary += "   ‚Ä¢ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–π—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∫—É—Ä—Å—ã\n"
            summary += "   ‚Ä¢ –ü—Ä–∞–∫—Ç–∏–∫—É–π—Ç–µ –Ω–∞–≤—ã–∫–∏ –≤ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω–æ–π —Ä–∞–±–æ—Ç–µ\n\n"
        
        high_score_comps = [comp for comp, data in competency_scores.items() if data['score'] >= 8]
        if high_score_comps:
            summary += "üéâ **–°–ò–õ–¨–ù–´–ï –°–¢–û–†–û–ù–´:**\n"
            summary += f"   ‚Ä¢ –û—Ç–ª–∏—á–Ω–æ —Ä–∞–∑–≤–∏—Ç—ã: {', '.join(high_score_comps[:3])}\n"
            summary += "   ‚Ä¢ –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –≤—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å\n\n"
        
        return summary

    def _create_detailed_report(self, analysis: Dict, trans_file_path: str, triggers_file_path: str) -> str:
        """–°–æ–∑–¥–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ REPORT.txt"""
        report = "üéØ –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ö–û–ú–ü–ï–¢–ï–ù–¶–ò–ô\n\n"
        report += "=" * 80 + "\n"
        report += f"üìÅ –§–∞–π–ª –≤—Å—Ç—Ä–µ—á–∏: {trans_file_path}\n"
        report += f"üìÅ –§–∞–π–ª —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤: {triggers_file_path}\n"
        report += f"üìÖ –î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        report += "=" * 80 + "\n\n"
        
        # –ê–Ω–∞–ª–∏–∑ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π
        for comp, data in analysis.items():
            if comp == '_detailed_stats':
                continue
            
            # –ü–æ–¥—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ –±–∞–ª–ª–∞ –ø–æ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏
            indicator_scores = []
            for indicator, ind_data in data["indicators"].items():
                indicator_scores.append(ind_data['score'])
            
            if indicator_scores:
                avg_score = sum(indicator_scores) / len(indicator_scores)
            else:
                avg_score = 0
            
            report += f"üèÜ {comp} - —Å—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª {avg_score:.1f}\n"
            report += "=" * 80 + "\n\n"
            
            for indicator, ind_data in data["indicators"].items():
                report += f"üìä {indicator} - {ind_data['score']:.1f} –±–∞–ª–ª–∞\n"
                report += f"   –ü–æ–∑–∏—Ç–∏–≤–Ω—ã—Ö: {ind_data['positive']['score']:.1f}\n"
                report += f"   –ù–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö: {ind_data['negative']['score']:.1f}\n"
                
                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                if ind_data['positive']['examples']:
                    report += "   \n   ‚úÖ –ù–ê–ô–î–ï–ù–û –í –¢–ï–ö–°–¢–ï (–ø–æ–∑–∏—Ç–∏–≤–Ω–æ–µ):\n"
                    for example in ind_data['positive']['examples']:
                        report += f"      ‚Ä¢ \"{example['found'][:200]}...\"\n"
                        report += f"        –ü–æ—Ö–æ–∂–µ –Ω–∞: \"{example['original'][:100]}...\"\n"
                        report += f"        –ë–∞–ª–ª: {example['score']:.1f}\n\n"
                
                if ind_data['negative']['examples']:
                    report += "   \n   ‚ùå –ù–ê–ô–î–ï–ù–û –í –¢–ï–ö–°–¢–ï (–Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–µ):\n"
                    for example in ind_data['negative']['examples']:
                        report += f"      ‚Ä¢ \"{example['found'][:200]}...\"\n"
                        report += f"        –ü–æ—Ö–æ–∂–µ –Ω–∞: \"{example['original'][:100]}...\"\n"
                        report += f"        –ë–∞–ª–ª: -{example['score']:.1f}\n"
                        if 'advice' in example:
                            report += f"        –°–æ–≤–µ—Ç: {example['advice']}\n"
                        report += "\n"
                
                # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫—É—Ä—Å–∞–º
                if ind_data['courses']:
                    report += "   üí° –†–ï–ö–û–ú–ï–ù–î–û–í–ê–ù–ù–´–ï –ö–£–†–°–´:\n"
                    for course in ind_data['courses']:
                        report += f"      ‚Ä¢ {course}\n"
                    if ind_data['negative']['score'] > 0:
                        report += f"      –û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: –Ω–∞–π–¥–µ–Ω –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π —Ç—Ä–∏–≥–≥–µ—Ä (–±–∞–ª–ª: -{ind_data['negative']['score']:.1f})\n"
                    report += "\n"
                
                report += "-" * 70 + "\n\n"
            
            report += "\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏
        if '_detailed_stats' in analysis:
            stats = analysis['_detailed_stats']
            report += "üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò:\n"
            report += "=" * 80 + "\n"
            report += f"   –í—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {stats.get('processing_time', 0):.1f}—Å\n"
            report += f"   –í—Å–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–π: {stats.get('total_comparisons', 0)}\n"
            report += f"   –¢–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {stats.get('tokens_used', 0)}\n"
            report += f"   –†–∞–∑–º–µ—Ä –∫—ç—à–∞: {stats.get('cache_size', 0)}\n"
            report += f"   –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {stats.get('sentences_processed', 0)}\n"
            report += "=" * 80 + "\n\n"
        
        return report

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ Telegram –±–æ—Ç–µ
async def analyze_competencies_async(trans_file: str, triggers_file: str) -> Tuple[str, str]:
    """
    –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π
    
    Args:
        trans_file: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Ç–µ–∫—Å—Ç–æ–º –≤—Å—Ç—Ä–µ—á–∏
        triggers_file: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å —Ç—Ä–∏–≥–≥–µ—Ä–∞–º–∏
        
    Returns:
        Tuple[str, str]: (–ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç, –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ)
    """
    analyzer = CompetencyAnalyzer()
    try:
        await analyzer.initialize()
        return await analyzer.analyze_competencies(trans_file, triggers_file)
    finally:
        await analyzer.close() 